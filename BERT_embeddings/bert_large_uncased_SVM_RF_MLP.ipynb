{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_large_uncased_SVM_RF_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b7d9c646417341eaae9ddb77b4a9eef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1279e7849c8e4704ae4df8ad857f8098",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eace588239084b6ebf748fb6d85bdde3",
              "IPY_MODEL_338b7d4e297f45de81031ee908f9f636"
            ]
          }
        },
        "1279e7849c8e4704ae4df8ad857f8098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eace588239084b6ebf748fb6d85bdde3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4d981cfc8f7944a9957d6b9cfb889bf9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_95f6ee9020be4c27acee76ae554675ed"
          }
        },
        "338b7d4e297f45de81031ee908f9f636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8c8ec7a8c4a64d30bc6b6e8dbe52ccde",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 571/571 [00:00&lt;00:00, 9.01kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9096fdf3a25a4e42afa2d4529e3542b3"
          }
        },
        "4d981cfc8f7944a9957d6b9cfb889bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "95f6ee9020be4c27acee76ae554675ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8c8ec7a8c4a64d30bc6b6e8dbe52ccde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9096fdf3a25a4e42afa2d4529e3542b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "726fc741abab44c1a5bb9e612c86427c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ebd14b23f53a411fa917cceae5dfecca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a4f2d870c22448399f8468fe35988370",
              "IPY_MODEL_dfb2d901a4894f039013042f9cb37e5e"
            ]
          }
        },
        "ebd14b23f53a411fa917cceae5dfecca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4f2d870c22448399f8468fe35988370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ab6582dfaf734a278f7f1d7435b3fd01",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1344997306,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1344997306,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1ca959c0891d4dadb312817c486ef4b6"
          }
        },
        "dfb2d901a4894f039013042f9cb37e5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_80370232094843e98d8e7e2f08755d9d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.34G/1.34G [01:45&lt;00:00, 12.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73c471f91c0646d6b582bb511c7c5eca"
          }
        },
        "ab6582dfaf734a278f7f1d7435b3fd01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1ca959c0891d4dadb312817c486ef4b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "80370232094843e98d8e7e2f08755d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73c471f91c0646d6b582bb511c7c5eca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbBwB6nH1MBE",
        "outputId": "8bc40176-1b68-4251-fbb4-baf0575788ab"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "\n",
        "import torch\n",
        "!pip install transformers\n",
        "import transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 19.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFY-NCHu2GNk"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "BcviZNM22KpO",
        "outputId": "8a90bebe-65d8-4d40-914e-24bfb0e6e4b2"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-76826cdf-f6b8-4153-91fa-55a26e1dc4fd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-76826cdf-f6b8-4153-91fa-55a26e1dc4fd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving FA18_messages_with_gender_recipients_hand_lableled.xlsx to FA18_messages_with_gender_recipients_hand_lableled.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "j9J9tNtl3Cci",
        "outputId": "5735db8a-c8aa-4627-dfdf-4a3f22a8db52"
      },
      "source": [
        "# Read in excel file with recipient gender hand labeled \n",
        "df_fall_18_messages = pd.read_excel('FA18_messages_with_gender_recipients_hand_lableled.xlsx', index_col=0)\n",
        "print(df_fall_18_messages.shape)\n",
        "df_fall_18_messages.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5039, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User ID</th>\n",
              "      <th>Sender Race</th>\n",
              "      <th>Sender Gender</th>\n",
              "      <th>Recipient Gender</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42683026</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>Hey @Katie Poteet I know you said we should em...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25501571</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>@Mary Cassell I would email Dr. K anyway with ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>42683026</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>Ok, thanks a lot. I have the email typed but w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25501571</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>Upstairs from our lecture hall</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25501571</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>Sherman 207!!!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    User ID  ...                                               Text\n",
              "0  42683026  ...  Hey @Katie Poteet I know you said we should em...\n",
              "1  25501571  ...  @Mary Cassell I would email Dr. K anyway with ...\n",
              "2  42683026  ...  Ok, thanks a lot. I have the email typed but w...\n",
              "3  25501571  ...                     Upstairs from our lecture hall\n",
              "4  25501571  ...                                     Sherman 207!!!\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "Lsy4PAEc3Gkd",
        "outputId": "e5af65cd-ea58-4f65-d529-674a27f9cee9"
      },
      "source": [
        "# Encode recipient gender char as int and create as new column in dataframe\n",
        "# Remove rows where recipient's gender is unknown\n",
        "male_val = 0\n",
        "female_val = 1\n",
        "neutral_val = 2\n",
        "\n",
        "def encode(c):\n",
        "    if (c == 'M'):\n",
        "        return male_val\n",
        "    elif (c == 'F'):\n",
        "        return female_val\n",
        "    elif (c == 'N'):\n",
        "        return neutral_val\n",
        "\n",
        "# Add column in dataframe for encoded gender\n",
        "df_fall_18_messages['Recipient Gender Encoded'] = [encode(x) for x in df_fall_18_messages['Recipient Gender']]\n",
        "# Drop nas (some genders were unknown in the dataset)\n",
        "df_fall_18_messages = df_fall_18_messages[df_fall_18_messages['Recipient Gender Encoded'].notna()]\n",
        "\n",
        "data = [df_fall_18_messages['Text'], df_fall_18_messages['Recipient Gender Encoded']]\n",
        "\n",
        "headers = ['Text', 'Recipient Gender Encoded']\n",
        "\n",
        "df_fall_18_messages = pd.concat(data, axis=1, keys=headers)\n",
        "\n",
        "print(df_fall_18_messages.shape)\n",
        "df_fall_18_messages.head()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4872, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Recipient Gender Encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hey @Katie Poteet I know you said we should em...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@Mary Cassell I would email Dr. K anyway with ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ok, thanks a lot. I have the email typed but w...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Upstairs from our lecture hall</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sherman 207!!!</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Recipient Gender Encoded\n",
              "0  Hey @Katie Poteet I know you said we should em...                       1.0\n",
              "1  @Mary Cassell I would email Dr. K anyway with ...                       1.0\n",
              "2  Ok, thanks a lot. I have the email typed but w...                       1.0\n",
              "3                     Upstairs from our lecture hall                       1.0\n",
              "4                                     Sherman 207!!!                       1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbxv9AgM3I4s",
        "outputId": "36fcce5f-21a8-4690-c573-407f65218fbe"
      },
      "source": [
        "nltk.download('punkt')\n",
        "df_fall_18_sentences = pd.DataFrame(columns=['Sentence', 'Recipient Gender'])\n",
        "counter = 0\n",
        "for index, row in df_fall_18_messages.iterrows():\n",
        "    for sentence in nltk.tokenize.sent_tokenize(row['Text']):\n",
        "        values_to_add = {'Sentence': sentence, 'Recipient Gender': row['Recipient Gender Encoded']}\n",
        "        row_to_add = pd.Series(values_to_add, name = counter)\n",
        "        df_fall_18_sentences = df_fall_18_sentences.append(row_to_add)\n",
        "        counter += 1"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "iDSpa4AV3KpF",
        "outputId": "4a0f1210-4afa-4f68-810d-5024dfb42aeb"
      },
      "source": [
        "print(numpy.argwhere(numpy.isnan(numpy.array(df_fall_18_sentences['Recipient Gender']))))\n",
        "print(df_fall_18_sentences.shape)\n",
        "df_fall_18_sentences.head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "(6354, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Recipient Gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hey @Katie Poteet I know you said we should em...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I’m planning to do the Baltimore Community Too...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@Mary Cassell I would email Dr. K anyway with ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If not, it’s still good for her to know what y...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ok, thanks a lot.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence  Recipient Gender\n",
              "0  Hey @Katie Poteet I know you said we should em...               1.0\n",
              "1  I’m planning to do the Baltimore Community Too...               1.0\n",
              "2  @Mary Cassell I would email Dr. K anyway with ...               1.0\n",
              "3  If not, it’s still good for her to know what y...               1.0\n",
              "4                                  Ok, thanks a lot.               1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wxsvhju3Md3",
        "outputId": "1c2b1886-8aca-4144-e14b-ee289641f8ba"
      },
      "source": [
        "print(df_fall_18_sentences['Recipient Gender'].value_counts())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0    2918\n",
            "1.0    1794\n",
            "0.0    1642\n",
            "Name: Recipient Gender, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmSBLmAX3UQI"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df_fall_18_sentences['Sentence'].values\n",
        "labels = df_fall_18_sentences['Recipient Gender'].values"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY9-iiOp3ayy"
      },
      "source": [
        "# Initialize the tokenizer with a pretrained model\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI9qg1bc4RvP"
      },
      "source": [
        "# Convert the string \"granola bars\" to tokenized vocabulary IDs\n",
        "sentence_ids = df_fall_18_sentences['Sentence'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJlygKKM44Qh",
        "outputId": "650154a3-b49a-46ed-ef33-d233c80aa74a"
      },
      "source": [
        "max_len = 0\n",
        "for id_vector in sentence_ids.values:\n",
        "    if len(id_vector) > max_len:\n",
        "        max_len = len(id_vector)\n",
        "\n",
        "padded_ids = numpy.array([i + [0]*(max_len-len(i)) for i in sentence_ids.values])\n",
        "print(\"max id array length: \", max_len)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max id array length:  105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuwgM-mv5Lx9",
        "outputId": "ba701345-1971-42b9-dd8e-41c5ac16f04a"
      },
      "source": [
        "attention_mask = numpy.where(padded_ids != 0, 1, 0)\n",
        "print(attention_mask.shape)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6354, 105)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyDKAqeC5epO"
      },
      "source": [
        "# Convert lists to tensors\n",
        "ids_tensor = torch.LongTensor(padded_ids)\n",
        "attention_masks_tensor = torch.tensor(attention_mask)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWnTuD4x9cQV"
      },
      "source": [
        "ids_batches = torch.tensor_split(ids_tensor, 100)\n",
        "attention_masks_batches = torch.tensor_split(attention_masks_tensor, 100)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b7d9c646417341eaae9ddb77b4a9eef0",
            "1279e7849c8e4704ae4df8ad857f8098",
            "eace588239084b6ebf748fb6d85bdde3",
            "338b7d4e297f45de81031ee908f9f636",
            "4d981cfc8f7944a9957d6b9cfb889bf9",
            "95f6ee9020be4c27acee76ae554675ed",
            "8c8ec7a8c4a64d30bc6b6e8dbe52ccde",
            "9096fdf3a25a4e42afa2d4529e3542b3",
            "726fc741abab44c1a5bb9e612c86427c",
            "ebd14b23f53a411fa917cceae5dfecca",
            "a4f2d870c22448399f8468fe35988370",
            "dfb2d901a4894f039013042f9cb37e5e",
            "ab6582dfaf734a278f7f1d7435b3fd01",
            "1ca959c0891d4dadb312817c486ef4b6",
            "80370232094843e98d8e7e2f08755d9d",
            "73c471f91c0646d6b582bb511c7c5eca"
          ]
        },
        "id": "DnJKnpgt5uP3",
        "outputId": "60b929ea-07e7-4a75-b99f-8f836cd0370a"
      },
      "source": [
        "model = transformers.BertModel.from_pretrained('bert-large-uncased', output_hidden_states=True)\n",
        "# Set the device to GPU (cuda) if available, otherwise stick with CPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)\n",
        "model.eval()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7d9c646417341eaae9ddb77b4a9eef0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=571.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "726fc741abab44c1a5bb9e612c86427c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1344997306.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 1024)\n",
              "    (token_type_embeddings): Embedding(2, 1024)\n",
              "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (12): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (13): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (14): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (15): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (16): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (17): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (18): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (19): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (20): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (21): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (22): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (23): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lQwhuA86lz4"
      },
      "source": [
        "hidden_states = []\n",
        "for i in range(0,len(ids_batches)):\n",
        "  with torch.no_grad():\n",
        "    ids_batch = ids_batches[i].to(device)\n",
        "    attention_masks_batch = attention_masks_batches[i].to(device)\n",
        "    hidden_state = model(ids_batch, attention_mask=attention_masks_batch)\n",
        "    states = torch.mean(hidden_state[2][-2], dim=1).squeeze().cpu()\n",
        "    for sentence in states: \n",
        "      hidden_states.append(sentence)\n",
        "    ids_batch.cpu()\n",
        "    attention_masks_batch.cpu()\n",
        "    del hidden_state, ids_batch, attention_masks_batch\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx-7Y8WfOXKs",
        "outputId": "b98ca8cd-c4ab-46c3-8138-218baf9e07a5"
      },
      "source": [
        "print(\"len(hidden_states): \",len(hidden_states))\n",
        "print(\"hidden_states[0].shape: \",hidden_states[0].shape)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(hidden_states):  6354\n",
            "hidden_states[0].shape:  torch.Size([1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFPqhRP-6Aqk"
      },
      "source": [
        "numpy_array = []\n",
        "for tensor in hidden_states:\n",
        "  numpy_array.append(tensor.numpy())\n",
        "\n",
        "X = numpy.array(numpy_array)\n",
        "y = df_fall_18_sentences[\"Recipient Gender\"]"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "2x5HLsbOVfvk",
        "outputId": "60469376-6d0e-4192-8983-80efa73091ef"
      },
      "source": [
        "df_features = pd.DataFrame(X)\n",
        "df_features.sample(10)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>984</th>\n",
              "      <th>985</th>\n",
              "      <th>986</th>\n",
              "      <th>987</th>\n",
              "      <th>988</th>\n",
              "      <th>989</th>\n",
              "      <th>990</th>\n",
              "      <th>991</th>\n",
              "      <th>992</th>\n",
              "      <th>993</th>\n",
              "      <th>994</th>\n",
              "      <th>995</th>\n",
              "      <th>996</th>\n",
              "      <th>997</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "      <th>1000</th>\n",
              "      <th>1001</th>\n",
              "      <th>1002</th>\n",
              "      <th>1003</th>\n",
              "      <th>1004</th>\n",
              "      <th>1005</th>\n",
              "      <th>1006</th>\n",
              "      <th>1007</th>\n",
              "      <th>1008</th>\n",
              "      <th>1009</th>\n",
              "      <th>1010</th>\n",
              "      <th>1011</th>\n",
              "      <th>1012</th>\n",
              "      <th>1013</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3651</th>\n",
              "      <td>0.066870</td>\n",
              "      <td>0.163269</td>\n",
              "      <td>-0.403746</td>\n",
              "      <td>0.075479</td>\n",
              "      <td>-0.257505</td>\n",
              "      <td>-0.541977</td>\n",
              "      <td>0.499186</td>\n",
              "      <td>0.641747</td>\n",
              "      <td>-0.208617</td>\n",
              "      <td>0.222774</td>\n",
              "      <td>0.756759</td>\n",
              "      <td>-0.285039</td>\n",
              "      <td>-0.508327</td>\n",
              "      <td>-0.158203</td>\n",
              "      <td>-0.304711</td>\n",
              "      <td>-0.286809</td>\n",
              "      <td>-0.679662</td>\n",
              "      <td>-0.044460</td>\n",
              "      <td>-1.102706</td>\n",
              "      <td>-0.550204</td>\n",
              "      <td>-1.118458</td>\n",
              "      <td>0.267884</td>\n",
              "      <td>0.130370</td>\n",
              "      <td>0.093148</td>\n",
              "      <td>-0.346738</td>\n",
              "      <td>0.448389</td>\n",
              "      <td>0.616437</td>\n",
              "      <td>0.455947</td>\n",
              "      <td>-0.355126</td>\n",
              "      <td>0.857983</td>\n",
              "      <td>-0.118844</td>\n",
              "      <td>-0.276067</td>\n",
              "      <td>0.160924</td>\n",
              "      <td>-0.380897</td>\n",
              "      <td>-0.237446</td>\n",
              "      <td>-0.080460</td>\n",
              "      <td>-0.190830</td>\n",
              "      <td>0.210290</td>\n",
              "      <td>0.283276</td>\n",
              "      <td>-0.858560</td>\n",
              "      <td>...</td>\n",
              "      <td>0.542744</td>\n",
              "      <td>-0.530739</td>\n",
              "      <td>0.027362</td>\n",
              "      <td>0.633896</td>\n",
              "      <td>-0.113939</td>\n",
              "      <td>0.100175</td>\n",
              "      <td>-0.299853</td>\n",
              "      <td>-0.301248</td>\n",
              "      <td>0.202061</td>\n",
              "      <td>-0.964025</td>\n",
              "      <td>0.022835</td>\n",
              "      <td>0.488384</td>\n",
              "      <td>0.457700</td>\n",
              "      <td>0.516489</td>\n",
              "      <td>-0.535366</td>\n",
              "      <td>-0.001454</td>\n",
              "      <td>-0.808053</td>\n",
              "      <td>0.888571</td>\n",
              "      <td>-0.009379</td>\n",
              "      <td>0.350976</td>\n",
              "      <td>0.242495</td>\n",
              "      <td>-0.073918</td>\n",
              "      <td>0.491848</td>\n",
              "      <td>-0.399806</td>\n",
              "      <td>-0.368131</td>\n",
              "      <td>-0.687512</td>\n",
              "      <td>-0.945844</td>\n",
              "      <td>-0.065388</td>\n",
              "      <td>0.051136</td>\n",
              "      <td>-0.212381</td>\n",
              "      <td>-0.236759</td>\n",
              "      <td>0.190823</td>\n",
              "      <td>-1.015455</td>\n",
              "      <td>-0.520372</td>\n",
              "      <td>-0.574559</td>\n",
              "      <td>0.346280</td>\n",
              "      <td>0.477189</td>\n",
              "      <td>0.371178</td>\n",
              "      <td>-0.203502</td>\n",
              "      <td>0.315431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6199</th>\n",
              "      <td>-0.220386</td>\n",
              "      <td>-0.295834</td>\n",
              "      <td>-0.452585</td>\n",
              "      <td>0.088207</td>\n",
              "      <td>0.317727</td>\n",
              "      <td>-0.351356</td>\n",
              "      <td>0.699016</td>\n",
              "      <td>0.314967</td>\n",
              "      <td>0.174824</td>\n",
              "      <td>0.682983</td>\n",
              "      <td>0.524389</td>\n",
              "      <td>0.309149</td>\n",
              "      <td>-0.139579</td>\n",
              "      <td>-0.529784</td>\n",
              "      <td>-0.014219</td>\n",
              "      <td>-0.204006</td>\n",
              "      <td>-1.051198</td>\n",
              "      <td>-0.031390</td>\n",
              "      <td>-0.350680</td>\n",
              "      <td>-0.505080</td>\n",
              "      <td>-0.959808</td>\n",
              "      <td>0.552336</td>\n",
              "      <td>0.236779</td>\n",
              "      <td>-0.043628</td>\n",
              "      <td>-0.148407</td>\n",
              "      <td>0.464835</td>\n",
              "      <td>0.624493</td>\n",
              "      <td>0.190774</td>\n",
              "      <td>-0.216054</td>\n",
              "      <td>1.325830</td>\n",
              "      <td>-0.088712</td>\n",
              "      <td>-0.048982</td>\n",
              "      <td>-0.203581</td>\n",
              "      <td>-0.430034</td>\n",
              "      <td>-0.468901</td>\n",
              "      <td>-0.180202</td>\n",
              "      <td>-0.331414</td>\n",
              "      <td>-0.140633</td>\n",
              "      <td>0.196253</td>\n",
              "      <td>-0.298338</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.120078</td>\n",
              "      <td>0.093168</td>\n",
              "      <td>0.093870</td>\n",
              "      <td>0.115006</td>\n",
              "      <td>0.342475</td>\n",
              "      <td>-0.148241</td>\n",
              "      <td>-0.389297</td>\n",
              "      <td>-0.047737</td>\n",
              "      <td>0.337013</td>\n",
              "      <td>-0.426861</td>\n",
              "      <td>-0.075803</td>\n",
              "      <td>0.127580</td>\n",
              "      <td>0.260494</td>\n",
              "      <td>0.722250</td>\n",
              "      <td>-0.325418</td>\n",
              "      <td>0.539885</td>\n",
              "      <td>-0.230485</td>\n",
              "      <td>0.589567</td>\n",
              "      <td>0.108886</td>\n",
              "      <td>0.256116</td>\n",
              "      <td>0.446043</td>\n",
              "      <td>-0.322784</td>\n",
              "      <td>0.186939</td>\n",
              "      <td>-0.222963</td>\n",
              "      <td>-0.151011</td>\n",
              "      <td>-0.557733</td>\n",
              "      <td>-0.823095</td>\n",
              "      <td>-0.080740</td>\n",
              "      <td>-0.235266</td>\n",
              "      <td>-0.468861</td>\n",
              "      <td>0.277236</td>\n",
              "      <td>0.135407</td>\n",
              "      <td>-0.130578</td>\n",
              "      <td>-0.207770</td>\n",
              "      <td>-0.379104</td>\n",
              "      <td>0.588982</td>\n",
              "      <td>0.332728</td>\n",
              "      <td>0.435084</td>\n",
              "      <td>0.201629</td>\n",
              "      <td>-0.074311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3769</th>\n",
              "      <td>0.269661</td>\n",
              "      <td>0.022652</td>\n",
              "      <td>-0.354637</td>\n",
              "      <td>0.096200</td>\n",
              "      <td>-0.744355</td>\n",
              "      <td>-0.003915</td>\n",
              "      <td>0.519501</td>\n",
              "      <td>0.327058</td>\n",
              "      <td>0.088421</td>\n",
              "      <td>0.329862</td>\n",
              "      <td>0.478312</td>\n",
              "      <td>0.040448</td>\n",
              "      <td>-0.506069</td>\n",
              "      <td>-0.346776</td>\n",
              "      <td>-0.368780</td>\n",
              "      <td>0.046186</td>\n",
              "      <td>-0.775515</td>\n",
              "      <td>-0.173055</td>\n",
              "      <td>-0.885540</td>\n",
              "      <td>-0.748660</td>\n",
              "      <td>-1.016615</td>\n",
              "      <td>0.532498</td>\n",
              "      <td>0.120215</td>\n",
              "      <td>-0.106073</td>\n",
              "      <td>-0.128884</td>\n",
              "      <td>0.481302</td>\n",
              "      <td>0.523277</td>\n",
              "      <td>0.335486</td>\n",
              "      <td>0.092061</td>\n",
              "      <td>0.645932</td>\n",
              "      <td>0.325257</td>\n",
              "      <td>0.197373</td>\n",
              "      <td>0.327042</td>\n",
              "      <td>0.179770</td>\n",
              "      <td>0.019174</td>\n",
              "      <td>-0.831648</td>\n",
              "      <td>-0.565319</td>\n",
              "      <td>0.039820</td>\n",
              "      <td>0.512748</td>\n",
              "      <td>-0.380152</td>\n",
              "      <td>...</td>\n",
              "      <td>0.514447</td>\n",
              "      <td>-0.207638</td>\n",
              "      <td>-0.021799</td>\n",
              "      <td>0.316468</td>\n",
              "      <td>-0.005863</td>\n",
              "      <td>0.144733</td>\n",
              "      <td>-0.562108</td>\n",
              "      <td>0.282095</td>\n",
              "      <td>0.180377</td>\n",
              "      <td>-1.162781</td>\n",
              "      <td>-0.245090</td>\n",
              "      <td>0.730553</td>\n",
              "      <td>-0.136688</td>\n",
              "      <td>0.006623</td>\n",
              "      <td>-0.396603</td>\n",
              "      <td>0.102812</td>\n",
              "      <td>-0.254192</td>\n",
              "      <td>0.804935</td>\n",
              "      <td>-0.427988</td>\n",
              "      <td>0.245154</td>\n",
              "      <td>-0.014137</td>\n",
              "      <td>-0.227716</td>\n",
              "      <td>-0.169562</td>\n",
              "      <td>-0.385157</td>\n",
              "      <td>-0.767106</td>\n",
              "      <td>-0.175135</td>\n",
              "      <td>-0.820127</td>\n",
              "      <td>0.314552</td>\n",
              "      <td>0.171279</td>\n",
              "      <td>-0.322376</td>\n",
              "      <td>0.127757</td>\n",
              "      <td>0.050948</td>\n",
              "      <td>-0.190891</td>\n",
              "      <td>-0.336748</td>\n",
              "      <td>-0.533354</td>\n",
              "      <td>0.278270</td>\n",
              "      <td>0.391152</td>\n",
              "      <td>0.207831</td>\n",
              "      <td>0.361243</td>\n",
              "      <td>0.077149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1511</th>\n",
              "      <td>0.352884</td>\n",
              "      <td>0.112484</td>\n",
              "      <td>-0.419604</td>\n",
              "      <td>0.214897</td>\n",
              "      <td>-0.071553</td>\n",
              "      <td>0.040929</td>\n",
              "      <td>0.617969</td>\n",
              "      <td>0.514089</td>\n",
              "      <td>0.050548</td>\n",
              "      <td>0.463280</td>\n",
              "      <td>0.397221</td>\n",
              "      <td>0.416921</td>\n",
              "      <td>-0.240649</td>\n",
              "      <td>-0.126859</td>\n",
              "      <td>-0.279789</td>\n",
              "      <td>0.043642</td>\n",
              "      <td>-1.016060</td>\n",
              "      <td>0.303402</td>\n",
              "      <td>-0.644140</td>\n",
              "      <td>-0.677837</td>\n",
              "      <td>-1.245627</td>\n",
              "      <td>0.810397</td>\n",
              "      <td>-0.012902</td>\n",
              "      <td>-0.265110</td>\n",
              "      <td>-0.358651</td>\n",
              "      <td>0.275088</td>\n",
              "      <td>0.980859</td>\n",
              "      <td>0.369988</td>\n",
              "      <td>0.124009</td>\n",
              "      <td>0.905679</td>\n",
              "      <td>-0.261214</td>\n",
              "      <td>0.124350</td>\n",
              "      <td>0.161081</td>\n",
              "      <td>-0.295580</td>\n",
              "      <td>-0.276900</td>\n",
              "      <td>-0.183476</td>\n",
              "      <td>-0.476685</td>\n",
              "      <td>-0.388481</td>\n",
              "      <td>0.158596</td>\n",
              "      <td>-0.828662</td>\n",
              "      <td>...</td>\n",
              "      <td>0.354505</td>\n",
              "      <td>-0.694773</td>\n",
              "      <td>0.150601</td>\n",
              "      <td>0.013198</td>\n",
              "      <td>0.148926</td>\n",
              "      <td>0.631275</td>\n",
              "      <td>-0.332664</td>\n",
              "      <td>-0.281982</td>\n",
              "      <td>0.291761</td>\n",
              "      <td>-0.602839</td>\n",
              "      <td>0.198136</td>\n",
              "      <td>0.511919</td>\n",
              "      <td>-0.113165</td>\n",
              "      <td>-0.172876</td>\n",
              "      <td>-1.074681</td>\n",
              "      <td>0.246023</td>\n",
              "      <td>-0.689226</td>\n",
              "      <td>0.713362</td>\n",
              "      <td>-0.277529</td>\n",
              "      <td>0.167392</td>\n",
              "      <td>0.250222</td>\n",
              "      <td>-0.131525</td>\n",
              "      <td>-0.344269</td>\n",
              "      <td>-0.361422</td>\n",
              "      <td>-0.288399</td>\n",
              "      <td>-0.915264</td>\n",
              "      <td>-1.550574</td>\n",
              "      <td>0.878317</td>\n",
              "      <td>-0.288498</td>\n",
              "      <td>0.036145</td>\n",
              "      <td>-0.007674</td>\n",
              "      <td>0.000833</td>\n",
              "      <td>-0.799588</td>\n",
              "      <td>-0.389696</td>\n",
              "      <td>-0.340072</td>\n",
              "      <td>1.003019</td>\n",
              "      <td>0.265057</td>\n",
              "      <td>0.823295</td>\n",
              "      <td>-0.197020</td>\n",
              "      <td>0.509465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3821</th>\n",
              "      <td>-0.120604</td>\n",
              "      <td>0.095575</td>\n",
              "      <td>-0.573367</td>\n",
              "      <td>0.079838</td>\n",
              "      <td>-0.590268</td>\n",
              "      <td>-0.286333</td>\n",
              "      <td>0.551996</td>\n",
              "      <td>0.375310</td>\n",
              "      <td>-0.276841</td>\n",
              "      <td>0.650637</td>\n",
              "      <td>0.712253</td>\n",
              "      <td>0.518440</td>\n",
              "      <td>-0.394218</td>\n",
              "      <td>-0.302793</td>\n",
              "      <td>-0.707579</td>\n",
              "      <td>0.355280</td>\n",
              "      <td>-0.824044</td>\n",
              "      <td>-0.241748</td>\n",
              "      <td>-0.856117</td>\n",
              "      <td>-0.005946</td>\n",
              "      <td>-0.674003</td>\n",
              "      <td>0.507445</td>\n",
              "      <td>-0.211615</td>\n",
              "      <td>-0.187580</td>\n",
              "      <td>-0.080972</td>\n",
              "      <td>0.726302</td>\n",
              "      <td>0.755127</td>\n",
              "      <td>-0.222233</td>\n",
              "      <td>-0.110934</td>\n",
              "      <td>0.980640</td>\n",
              "      <td>-0.092540</td>\n",
              "      <td>-0.139034</td>\n",
              "      <td>-0.154259</td>\n",
              "      <td>0.374760</td>\n",
              "      <td>-0.229334</td>\n",
              "      <td>0.023262</td>\n",
              "      <td>-0.681072</td>\n",
              "      <td>-0.495110</td>\n",
              "      <td>0.205020</td>\n",
              "      <td>-0.879266</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.292722</td>\n",
              "      <td>-0.394163</td>\n",
              "      <td>0.038172</td>\n",
              "      <td>0.205017</td>\n",
              "      <td>-0.146292</td>\n",
              "      <td>0.142787</td>\n",
              "      <td>-0.208705</td>\n",
              "      <td>-0.204810</td>\n",
              "      <td>0.416044</td>\n",
              "      <td>-0.664836</td>\n",
              "      <td>0.976766</td>\n",
              "      <td>0.549964</td>\n",
              "      <td>0.058096</td>\n",
              "      <td>-0.138208</td>\n",
              "      <td>-0.620657</td>\n",
              "      <td>-0.055939</td>\n",
              "      <td>-0.128891</td>\n",
              "      <td>0.697687</td>\n",
              "      <td>-0.324985</td>\n",
              "      <td>0.396631</td>\n",
              "      <td>0.205023</td>\n",
              "      <td>-0.066524</td>\n",
              "      <td>0.079329</td>\n",
              "      <td>0.225500</td>\n",
              "      <td>-1.075613</td>\n",
              "      <td>-0.531108</td>\n",
              "      <td>-1.413394</td>\n",
              "      <td>0.323403</td>\n",
              "      <td>0.207006</td>\n",
              "      <td>0.039406</td>\n",
              "      <td>-0.016273</td>\n",
              "      <td>-0.032924</td>\n",
              "      <td>-0.067758</td>\n",
              "      <td>-0.507730</td>\n",
              "      <td>-0.315683</td>\n",
              "      <td>0.832043</td>\n",
              "      <td>0.708360</td>\n",
              "      <td>0.450490</td>\n",
              "      <td>-0.238888</td>\n",
              "      <td>-0.555722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>405</th>\n",
              "      <td>-0.012661</td>\n",
              "      <td>0.352159</td>\n",
              "      <td>-0.198422</td>\n",
              "      <td>-0.163729</td>\n",
              "      <td>-0.257296</td>\n",
              "      <td>-0.016296</td>\n",
              "      <td>0.773684</td>\n",
              "      <td>0.302394</td>\n",
              "      <td>-0.067880</td>\n",
              "      <td>0.379982</td>\n",
              "      <td>0.171168</td>\n",
              "      <td>0.007708</td>\n",
              "      <td>-0.051225</td>\n",
              "      <td>-0.137707</td>\n",
              "      <td>-0.394448</td>\n",
              "      <td>0.052070</td>\n",
              "      <td>-0.465532</td>\n",
              "      <td>-0.352178</td>\n",
              "      <td>-0.539660</td>\n",
              "      <td>-0.669610</td>\n",
              "      <td>-1.076378</td>\n",
              "      <td>1.028191</td>\n",
              "      <td>0.172504</td>\n",
              "      <td>0.093299</td>\n",
              "      <td>-0.465745</td>\n",
              "      <td>0.549169</td>\n",
              "      <td>0.248055</td>\n",
              "      <td>0.218548</td>\n",
              "      <td>-0.257646</td>\n",
              "      <td>0.649521</td>\n",
              "      <td>-0.022028</td>\n",
              "      <td>-0.196851</td>\n",
              "      <td>-0.126281</td>\n",
              "      <td>-0.196102</td>\n",
              "      <td>0.080643</td>\n",
              "      <td>-0.060343</td>\n",
              "      <td>0.081908</td>\n",
              "      <td>-0.099444</td>\n",
              "      <td>-0.127919</td>\n",
              "      <td>-0.689078</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026966</td>\n",
              "      <td>0.234513</td>\n",
              "      <td>0.289206</td>\n",
              "      <td>0.304206</td>\n",
              "      <td>-0.381508</td>\n",
              "      <td>0.613576</td>\n",
              "      <td>-0.126377</td>\n",
              "      <td>-0.384771</td>\n",
              "      <td>0.168447</td>\n",
              "      <td>-0.791148</td>\n",
              "      <td>-0.203625</td>\n",
              "      <td>0.514674</td>\n",
              "      <td>0.129306</td>\n",
              "      <td>0.087647</td>\n",
              "      <td>-0.283179</td>\n",
              "      <td>-0.072523</td>\n",
              "      <td>0.002349</td>\n",
              "      <td>-0.046441</td>\n",
              "      <td>0.205007</td>\n",
              "      <td>-0.065332</td>\n",
              "      <td>0.359753</td>\n",
              "      <td>-0.516547</td>\n",
              "      <td>0.233400</td>\n",
              "      <td>-0.455034</td>\n",
              "      <td>-0.032396</td>\n",
              "      <td>-0.607479</td>\n",
              "      <td>0.376738</td>\n",
              "      <td>-0.583453</td>\n",
              "      <td>-0.138824</td>\n",
              "      <td>-0.166077</td>\n",
              "      <td>0.441809</td>\n",
              "      <td>-0.105403</td>\n",
              "      <td>-0.437154</td>\n",
              "      <td>-0.509070</td>\n",
              "      <td>-0.492340</td>\n",
              "      <td>-0.019125</td>\n",
              "      <td>-0.136009</td>\n",
              "      <td>0.200336</td>\n",
              "      <td>-0.337338</td>\n",
              "      <td>0.010982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2235</th>\n",
              "      <td>0.169683</td>\n",
              "      <td>-0.057661</td>\n",
              "      <td>-0.003001</td>\n",
              "      <td>-0.083185</td>\n",
              "      <td>0.239007</td>\n",
              "      <td>-0.076169</td>\n",
              "      <td>0.104054</td>\n",
              "      <td>0.514344</td>\n",
              "      <td>-0.324524</td>\n",
              "      <td>0.074584</td>\n",
              "      <td>0.397231</td>\n",
              "      <td>-0.131797</td>\n",
              "      <td>0.010391</td>\n",
              "      <td>-0.136811</td>\n",
              "      <td>-0.103671</td>\n",
              "      <td>-0.240081</td>\n",
              "      <td>-0.285476</td>\n",
              "      <td>0.024437</td>\n",
              "      <td>-0.361264</td>\n",
              "      <td>-0.410896</td>\n",
              "      <td>-0.417491</td>\n",
              "      <td>0.406585</td>\n",
              "      <td>-0.018313</td>\n",
              "      <td>-0.127682</td>\n",
              "      <td>-0.249138</td>\n",
              "      <td>0.006904</td>\n",
              "      <td>0.101979</td>\n",
              "      <td>0.060597</td>\n",
              "      <td>0.251758</td>\n",
              "      <td>0.794603</td>\n",
              "      <td>-0.214007</td>\n",
              "      <td>0.189926</td>\n",
              "      <td>-0.254150</td>\n",
              "      <td>-0.427641</td>\n",
              "      <td>0.334943</td>\n",
              "      <td>0.019161</td>\n",
              "      <td>0.214949</td>\n",
              "      <td>0.097021</td>\n",
              "      <td>-0.227896</td>\n",
              "      <td>-0.147326</td>\n",
              "      <td>...</td>\n",
              "      <td>0.284630</td>\n",
              "      <td>0.095267</td>\n",
              "      <td>0.199863</td>\n",
              "      <td>-0.141682</td>\n",
              "      <td>-0.286636</td>\n",
              "      <td>0.067915</td>\n",
              "      <td>0.145784</td>\n",
              "      <td>0.072009</td>\n",
              "      <td>0.066435</td>\n",
              "      <td>-0.632637</td>\n",
              "      <td>0.130296</td>\n",
              "      <td>-0.024517</td>\n",
              "      <td>-0.271770</td>\n",
              "      <td>0.156371</td>\n",
              "      <td>-0.171682</td>\n",
              "      <td>-0.008692</td>\n",
              "      <td>-0.251792</td>\n",
              "      <td>0.098531</td>\n",
              "      <td>0.179740</td>\n",
              "      <td>-0.059745</td>\n",
              "      <td>0.860868</td>\n",
              "      <td>-0.557584</td>\n",
              "      <td>0.247557</td>\n",
              "      <td>-0.166812</td>\n",
              "      <td>-0.036842</td>\n",
              "      <td>0.001141</td>\n",
              "      <td>-0.545132</td>\n",
              "      <td>0.145545</td>\n",
              "      <td>-0.252146</td>\n",
              "      <td>-0.149829</td>\n",
              "      <td>0.183842</td>\n",
              "      <td>0.020066</td>\n",
              "      <td>-0.034693</td>\n",
              "      <td>-0.096412</td>\n",
              "      <td>-0.121116</td>\n",
              "      <td>0.522678</td>\n",
              "      <td>0.223395</td>\n",
              "      <td>0.362745</td>\n",
              "      <td>0.178315</td>\n",
              "      <td>-0.052217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5253</th>\n",
              "      <td>0.223677</td>\n",
              "      <td>-0.036921</td>\n",
              "      <td>-0.184219</td>\n",
              "      <td>0.332416</td>\n",
              "      <td>-0.384482</td>\n",
              "      <td>-0.494139</td>\n",
              "      <td>0.279681</td>\n",
              "      <td>-0.009358</td>\n",
              "      <td>-0.322573</td>\n",
              "      <td>-0.176995</td>\n",
              "      <td>0.642210</td>\n",
              "      <td>0.299256</td>\n",
              "      <td>-0.616883</td>\n",
              "      <td>-0.181892</td>\n",
              "      <td>-0.382741</td>\n",
              "      <td>0.583622</td>\n",
              "      <td>-0.390376</td>\n",
              "      <td>-0.289084</td>\n",
              "      <td>-0.321444</td>\n",
              "      <td>-0.562650</td>\n",
              "      <td>-0.471309</td>\n",
              "      <td>0.128248</td>\n",
              "      <td>-0.089950</td>\n",
              "      <td>-0.007497</td>\n",
              "      <td>-0.543416</td>\n",
              "      <td>-0.253485</td>\n",
              "      <td>0.365843</td>\n",
              "      <td>0.482432</td>\n",
              "      <td>0.240517</td>\n",
              "      <td>1.029478</td>\n",
              "      <td>-0.811148</td>\n",
              "      <td>-0.509676</td>\n",
              "      <td>-0.131295</td>\n",
              "      <td>0.216126</td>\n",
              "      <td>0.124691</td>\n",
              "      <td>-0.187643</td>\n",
              "      <td>-0.008859</td>\n",
              "      <td>-0.041217</td>\n",
              "      <td>1.022017</td>\n",
              "      <td>-0.257930</td>\n",
              "      <td>...</td>\n",
              "      <td>0.328659</td>\n",
              "      <td>-0.529676</td>\n",
              "      <td>-0.049993</td>\n",
              "      <td>-0.159507</td>\n",
              "      <td>0.250558</td>\n",
              "      <td>-0.026344</td>\n",
              "      <td>0.002118</td>\n",
              "      <td>-0.636654</td>\n",
              "      <td>0.841395</td>\n",
              "      <td>-0.266647</td>\n",
              "      <td>0.423502</td>\n",
              "      <td>0.788553</td>\n",
              "      <td>-0.467527</td>\n",
              "      <td>-0.197005</td>\n",
              "      <td>-0.169125</td>\n",
              "      <td>0.060233</td>\n",
              "      <td>-0.851335</td>\n",
              "      <td>0.597466</td>\n",
              "      <td>0.000288</td>\n",
              "      <td>-0.104970</td>\n",
              "      <td>0.252700</td>\n",
              "      <td>-0.373809</td>\n",
              "      <td>0.281163</td>\n",
              "      <td>-0.266160</td>\n",
              "      <td>-0.985826</td>\n",
              "      <td>-0.324320</td>\n",
              "      <td>-0.883613</td>\n",
              "      <td>0.559668</td>\n",
              "      <td>-0.484071</td>\n",
              "      <td>-0.001094</td>\n",
              "      <td>-0.102840</td>\n",
              "      <td>-0.347539</td>\n",
              "      <td>-0.544415</td>\n",
              "      <td>-0.470093</td>\n",
              "      <td>-0.028003</td>\n",
              "      <td>0.230159</td>\n",
              "      <td>0.288507</td>\n",
              "      <td>0.754267</td>\n",
              "      <td>-0.918877</td>\n",
              "      <td>0.335643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1374</th>\n",
              "      <td>0.139111</td>\n",
              "      <td>-0.328311</td>\n",
              "      <td>-0.443329</td>\n",
              "      <td>-0.011408</td>\n",
              "      <td>-0.338601</td>\n",
              "      <td>-0.577362</td>\n",
              "      <td>0.383693</td>\n",
              "      <td>0.363590</td>\n",
              "      <td>-0.094373</td>\n",
              "      <td>0.334822</td>\n",
              "      <td>0.407998</td>\n",
              "      <td>0.245428</td>\n",
              "      <td>-0.676048</td>\n",
              "      <td>-0.007033</td>\n",
              "      <td>-0.409546</td>\n",
              "      <td>0.165888</td>\n",
              "      <td>-0.741465</td>\n",
              "      <td>-0.400321</td>\n",
              "      <td>-0.605823</td>\n",
              "      <td>-0.539188</td>\n",
              "      <td>-0.757722</td>\n",
              "      <td>0.270784</td>\n",
              "      <td>0.369250</td>\n",
              "      <td>-0.224685</td>\n",
              "      <td>-0.163710</td>\n",
              "      <td>0.200658</td>\n",
              "      <td>0.325679</td>\n",
              "      <td>0.493275</td>\n",
              "      <td>-0.393889</td>\n",
              "      <td>0.368763</td>\n",
              "      <td>-0.067844</td>\n",
              "      <td>-0.160921</td>\n",
              "      <td>0.309267</td>\n",
              "      <td>-0.358302</td>\n",
              "      <td>-0.384453</td>\n",
              "      <td>-0.362804</td>\n",
              "      <td>0.108807</td>\n",
              "      <td>0.490798</td>\n",
              "      <td>0.365428</td>\n",
              "      <td>0.105780</td>\n",
              "      <td>...</td>\n",
              "      <td>0.722623</td>\n",
              "      <td>-0.554305</td>\n",
              "      <td>0.224362</td>\n",
              "      <td>0.502679</td>\n",
              "      <td>-0.403191</td>\n",
              "      <td>-0.465468</td>\n",
              "      <td>-0.384049</td>\n",
              "      <td>-0.303138</td>\n",
              "      <td>0.190596</td>\n",
              "      <td>-0.771664</td>\n",
              "      <td>-0.537058</td>\n",
              "      <td>0.634824</td>\n",
              "      <td>0.243900</td>\n",
              "      <td>0.434410</td>\n",
              "      <td>-0.564198</td>\n",
              "      <td>0.046591</td>\n",
              "      <td>-0.429420</td>\n",
              "      <td>0.273463</td>\n",
              "      <td>0.341246</td>\n",
              "      <td>0.405886</td>\n",
              "      <td>-0.546261</td>\n",
              "      <td>-0.459473</td>\n",
              "      <td>-0.009994</td>\n",
              "      <td>0.151928</td>\n",
              "      <td>-0.264226</td>\n",
              "      <td>-0.216269</td>\n",
              "      <td>-0.534043</td>\n",
              "      <td>-0.276177</td>\n",
              "      <td>-0.215479</td>\n",
              "      <td>-0.244925</td>\n",
              "      <td>-0.051523</td>\n",
              "      <td>0.273136</td>\n",
              "      <td>-1.027448</td>\n",
              "      <td>0.221544</td>\n",
              "      <td>-0.628432</td>\n",
              "      <td>-0.027693</td>\n",
              "      <td>0.179381</td>\n",
              "      <td>-0.174447</td>\n",
              "      <td>-0.283712</td>\n",
              "      <td>0.219966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4326</th>\n",
              "      <td>-0.056528</td>\n",
              "      <td>-0.139449</td>\n",
              "      <td>-0.381845</td>\n",
              "      <td>-0.418211</td>\n",
              "      <td>0.079580</td>\n",
              "      <td>-0.714961</td>\n",
              "      <td>0.436179</td>\n",
              "      <td>0.866535</td>\n",
              "      <td>-0.195751</td>\n",
              "      <td>0.169444</td>\n",
              "      <td>0.045421</td>\n",
              "      <td>0.115125</td>\n",
              "      <td>-0.127561</td>\n",
              "      <td>-0.230101</td>\n",
              "      <td>-0.371042</td>\n",
              "      <td>-0.328595</td>\n",
              "      <td>-0.499915</td>\n",
              "      <td>0.142852</td>\n",
              "      <td>-0.742587</td>\n",
              "      <td>-0.599970</td>\n",
              "      <td>-1.160835</td>\n",
              "      <td>0.920885</td>\n",
              "      <td>0.044318</td>\n",
              "      <td>-0.079851</td>\n",
              "      <td>-0.591211</td>\n",
              "      <td>0.419930</td>\n",
              "      <td>0.326155</td>\n",
              "      <td>0.152688</td>\n",
              "      <td>-0.273260</td>\n",
              "      <td>0.801932</td>\n",
              "      <td>-0.536981</td>\n",
              "      <td>0.274729</td>\n",
              "      <td>-0.076643</td>\n",
              "      <td>-0.149731</td>\n",
              "      <td>-0.288415</td>\n",
              "      <td>-0.330777</td>\n",
              "      <td>0.594000</td>\n",
              "      <td>0.075220</td>\n",
              "      <td>-0.175124</td>\n",
              "      <td>-0.472461</td>\n",
              "      <td>...</td>\n",
              "      <td>0.023306</td>\n",
              "      <td>-0.021148</td>\n",
              "      <td>0.529478</td>\n",
              "      <td>0.172594</td>\n",
              "      <td>0.023384</td>\n",
              "      <td>-0.516173</td>\n",
              "      <td>-0.112347</td>\n",
              "      <td>-1.137334</td>\n",
              "      <td>0.162868</td>\n",
              "      <td>-0.505120</td>\n",
              "      <td>-0.257554</td>\n",
              "      <td>0.097263</td>\n",
              "      <td>0.403224</td>\n",
              "      <td>0.165572</td>\n",
              "      <td>-0.408740</td>\n",
              "      <td>-0.216621</td>\n",
              "      <td>-0.149432</td>\n",
              "      <td>-0.145729</td>\n",
              "      <td>-0.035169</td>\n",
              "      <td>0.469774</td>\n",
              "      <td>0.313791</td>\n",
              "      <td>-0.580898</td>\n",
              "      <td>-0.226188</td>\n",
              "      <td>0.015462</td>\n",
              "      <td>-0.245908</td>\n",
              "      <td>-0.251953</td>\n",
              "      <td>0.319434</td>\n",
              "      <td>-0.287962</td>\n",
              "      <td>-0.079726</td>\n",
              "      <td>0.006849</td>\n",
              "      <td>0.294159</td>\n",
              "      <td>0.270561</td>\n",
              "      <td>-0.180676</td>\n",
              "      <td>-0.535624</td>\n",
              "      <td>-0.347633</td>\n",
              "      <td>0.038574</td>\n",
              "      <td>0.428428</td>\n",
              "      <td>0.234302</td>\n",
              "      <td>-0.331836</td>\n",
              "      <td>-0.163873</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 1024 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2     ...      1021      1022      1023\n",
              "3651  0.066870  0.163269 -0.403746  ...  0.371178 -0.203502  0.315431\n",
              "6199 -0.220386 -0.295834 -0.452585  ...  0.435084  0.201629 -0.074311\n",
              "3769  0.269661  0.022652 -0.354637  ...  0.207831  0.361243  0.077149\n",
              "1511  0.352884  0.112484 -0.419604  ...  0.823295 -0.197020  0.509465\n",
              "3821 -0.120604  0.095575 -0.573367  ...  0.450490 -0.238888 -0.555722\n",
              "405  -0.012661  0.352159 -0.198422  ...  0.200336 -0.337338  0.010982\n",
              "2235  0.169683 -0.057661 -0.003001  ...  0.362745  0.178315 -0.052217\n",
              "5253  0.223677 -0.036921 -0.184219  ...  0.754267 -0.918877  0.335643\n",
              "1374  0.139111 -0.328311 -0.443329  ... -0.174447 -0.283712  0.219966\n",
              "4326 -0.056528 -0.139449 -0.381845  ...  0.234302 -0.331836 -0.163873\n",
              "\n",
              "[10 rows x 1024 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K68gPiRzVs5S",
        "outputId": "dd1d2094-183c-4bda-b3f7-f4ef5d63688a"
      },
      "source": [
        "# Print y shape, head, unique values and number of instances in each class\n",
        "print(y.shape)\n",
        "print(y.unique())\n",
        "print(y.value_counts())"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6354,)\n",
            "[1. 2. 0.]\n",
            "2.0    2918\n",
            "1.0    1794\n",
            "0.0    1642\n",
            "Name: Recipient Gender, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PF_KelLVvQr"
      },
      "source": [
        "# Feature extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Train/Test splitting\n",
        "from sklearn import model_selection\n",
        "\n",
        "# Hyper-parameter tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV,  GridSearchCV, cross_val_score\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Classifier evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, make_scorer, confusion_matrix, classification_report\n",
        "\n",
        "# Print confusion matrix\n",
        "import seaborn as sns"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsIFk6KnVyTJ",
        "outputId": "4cae819b-6c7d-482f-9a28-251f402a01bc"
      },
      "source": [
        "# split into training and test set\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5083, 1024)\n",
            "(5083,)\n",
            "(1271, 1024)\n",
            "(1271,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or69yK7wWgZx"
      },
      "source": [
        "svm_clf = SVC(C=10, gamma=0.1, probability=True)\n",
        "rf_clf = RandomForestClassifier(max_depth=50, min_samples_leaf=3, min_samples_split=3)\n",
        "mlp_clf = MLPClassifier(alpha=0.05, hidden_layer_sizes=(50, 100, 50))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90ttyo7yWm8Y",
        "outputId": "6c6cb674-7e54-43c9-f62a-fee1a32b24b3"
      },
      "source": [
        "svm_clf.fit(X_train, y_train)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "mlp_clf.fit(X_train, y_train)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.05, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(50, 100, 50), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs36_e2OWpzu"
      },
      "source": [
        "\n",
        "def print_results(clf, y_pred):\n",
        "    # Print accuracy score \n",
        "    print(\"Accuracy Score -> \",accuracy_score(y_pred, y_test)*100)\n",
        "    \n",
        "    # Print confusion matrix (heat map)\n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    \n",
        "    # Print avarage 10-fold cross validation accuracy and f1 scores \n",
        "    #svm_cv_accuracy_score = cross_val_score(clf, X_train, y_train, cv=10,scoring='accuracy')\n",
        "    #svm_cv_f1_score = cross_val_score(clf, X_train, y_train, cv=10,scoring='f1_macro')\n",
        "    #print(\"Mean cv accuracy: \", svm_cv_accuracy_score.mean())\n",
        "    #print(\"Mean cv f1_macro: \", svm_cv_f1_score.mean())"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "YwrRPyUeWrmr",
        "outputId": "2c946ab9-3f32-41a0-b95f-78f68536a1c8"
      },
      "source": [
        "# predict recipient gender and print prediction results\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "print_results(svm_clf, y_pred)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score ->  47.91502753737215\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.28      0.03      0.05       300\n",
            "         1.0       0.55      0.06      0.10       384\n",
            "         2.0       0.48      0.98      0.65       587\n",
            "\n",
            "    accuracy                           0.48      1271\n",
            "   macro avg       0.44      0.36      0.27      1271\n",
            "weighted avg       0.46      0.48      0.34      1271\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD4CAYAAADSIzzWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYyElEQVR4nO3de3hV5ZXH8e86CaBc5C5iAJERpdp6qagIiiJggVKhWjulWqGi6VRpsdNWqdZpq4zXjk61lZERC1gq3oVx0KqII6jcRJQioNHSyjUIiNzhnKz5I9uYQHLOCSR5cza/j89+sve7b4s8cWVl7fecY+6OiIjUvUToAEREDlVKwCIigSgBi4gEogQsIhKIErCISCD5tX2Dho06aJpFLUuYfo/WthdbnBU6hENC73VP2MFeY+8nH2Wdcxq06XLQ9zsY+j9XRCSQWq+ARUTqVEkqdARZUwIWkXhJJUNHkDUlYBGJFfeS0CFkTQlYROKlRAlYRCQMVcAiIoHoIZyISCCqgEVEwnDNghARCUQP4UREAlELQkQkED2EExEJRBWwiEggeggnIhKIHsKJiIThrh6wiEgY6gGLiASiFoSISCCqgEVEAkntDR1B1pSARSRe1IIQEQlELQgRkUBUAYuIBKIELCIShushnIhIIDnUA06EDkBEpEaVlGS/ZGBmK81siZktNrOF0VgrM3vJzD6IvraMxs3M7jOzIjN718y+mun6SsAiEi9ekv2SnT7ufqq7d4+2xwAz3b0rMDPaBhgIdI2WQmBcpgsrAYtIvNRgBVyFIcCkaH0SMLTc+GQvNRdoYWbt011ICVhE4qUaFbCZFZrZwnJL4b5XA140s7fK7Wvn7muj9XVAu2i9APi43LmrorEq6SGciMRLMvs3ZHf38cD4NIec4+6rzexI4CUzW77P+W5mfmCBKgFXMGrUSEZeOQwzY8LDf+b++yeEDikWHnzwbgYO7MuGDRs5/fT+FfaNHn01d955MwUFp7Bx4+ZAEeamRke35oT7R9GgbQtwZ+0jL7PmoRl0e/AnNP6nowHIb96Y5JYdLOr3c468+Bw6XDOk7PwmJ3ZiUf8b2L50ZaB/QS2pwVkQ7r46+lpsZs8AZwLrzay9u6+NWgzF0eGrgY7lTu8QjVVJCThy0oknMPLKYfTsNZg9e/by3HN/YsaMmXz44crQoeW8Rx55gnHjJjFhwr0Vxjt0aE+/fr35xz9WBYost3kyxUe/nsy2JX8jr8lhnPbinXz62rss/8EX3+cuv76C5Gc7ACh+eg7FT88BoHG3Tpw08efxS75QYy/EMLMmQMLdt0brFwK3ANOB4cAd0ddp0SnTgVFmNhU4C9hSrlVRKfWAI926Hcf8+YvZuXMXqVSK2a/NZejQgaHDioU5c+azefOn+43fddevuPHG23A/4L/gDml7ij9l25K/AZDavosdH6ym4VGtKhzT9htnU/zMnP3OPfKbvdjw7Bt1Emedq7lZEO2AOWb2DjAf+F93f4HSxNvfzD4A+kXbADOAj4Ai4L+BazLdIGMFbGbdKH2693kzeTUw3d2XZTo3lyx9bwW33HIDrVq1YOfOXQwYcAFvLXo3dFixNXhwf9asWceSJbH6MQqmUce2NP3ysWxd9EHZWPMeX2LPJ1vY9bd1+x3fdkhPlo64qy5DrDs1VAG7+0fAKZWMbwT6VjLuwLXVuUfaBGxmNwDDgKmU/gaA0r7Go2Y21d3vqPLkHLN8eRF3//YBZvzvn9m+fQfvvLuUVCp3Plsqlxx++GFcf/0oBg++PHQosZBofBgnPvQzPvy3P5LatrNsvO03z6m0+m122nGU7NzDjuUf77cvFnLolXCZKuCRwEnuXuHF1WZ2D7CUL0pv9tlfSOlEZPLyWpDIa1IDoda+iROnMnHiVABuveUGVq1O276RA9SlyzF07tyRBQteAKCgoD1z587gnHMuYv36DYGjyy2Wn8eJE35K8dOz2Thj/hc78hK0GXQmiy68Yb9z2g7tVWlijo1qzIIILVMCLgGOBv6+z3j7aF+lyk/taNioQ840+Nq2bc2GDRvp2PFohg4dyDnnXhQ6pFhaunQFnTp98SrNFStep2fPwZoFcQCOv/eH7PhgNasffK7CeMveJ7OjaA171m6qeIIZbS/qyTtDbq7DKOtYDj1TyJSArwNmRs3mz/9e6QQcB4yqzcBCeGzqeFq3bsnevUl+PPomtmz5LHRIsTB58v2ce+7ZtGnTkqKieYwdew8TJz4WOqycd8SZ3Wh36Xlse+/vfPXluwH42+1/ZvPMt2k7tBcbKqlym5/9JXav+YRd/yjeb19s5NDbUVqmJ9BmlqB07lv5h3AL3D2rBmkuVcC5KmGazFLbXmxxVugQDgm91z1hB3uNnVNuzjrnHH7ZrQd9v4ORcRaEu5cAc+sgFhGRgxejh3AiIrklh2YvKQGLSLzkUA9YCVhE4kUJWEQkEPWARUTC8JLcmXilBCwi8aIWhIhIIJoFISISiCpgEZFAlIBFRAKJ0ZvxiIjkFlXAIiKBaBqaiEggmgUhIhKGqwUhIhKIWhAiIoHovSBERAJRBSwiEkhSD+FERMJQC0JEJBC1IEREwtA0NBGRUFQBi4gEkkMJOBE6ABGRGpVKZb9kwczyzOxtM3su2j7WzOaZWZGZPWZmDaPxRtF2UbS/c6ZrKwGLSKx4iWe9ZGk0sKzc9p3Ave5+HLAZGBmNjwQ2R+P3RselpQQsIvFS4tkvGZhZB+DrwEPRtgEXAE9Gh0wChkbrQ6Jtov19o+OrpAQsIvFSUpL1YmaFZraw3FK4z9X+E7ge+HxqRWvgU3dPRturgIJovQD4GCDavyU6vkp6CCci8VKNh3DuPh4YX9k+MxsMFLv7W2Z2fs0EV5ESsIjES83NgugFXGRmg4DDgCOA3wEtzCw/qnI7AKuj41cDHYFVZpYPNAc2pruBWhAiEiueKsl6SXsd91+4ewd37wx8B3jF3S8DZgHfig4bDkyL1qdH20T7X3FP/wF1tV4BN8hTkV3b8ky/R2tb1+M+CR2CZKv25wHfAEw1s7HA28CEaHwC8IiZFQGbKE3aaSk7ikisVGN6WfbXdH8VeDVa/wg4s5JjdgGXVue6SsAiEi859Eo4JWARiZfceS8eJWARiRdP5k4GVgIWkXjJnfyrBCwi8VIbD+FqixKwiMSLKmARkTBUAYuIhKIKWEQkjLL3KcsBSsAiEis59Kn0SsAiEjNKwCIiYagCFhEJRAlYRCQQT6X9GLZ6RQlYRGJFFbCISCBeogpYRCQIVcAiIoG4qwIWEQlCFbCISCAlmgUhIhKGHsKJiASiBCwiEojnztsBKwGLSLyoAhYRCUTT0EREAklpFoSISBiqgEVEAlEPWEQkkFyaBZEIHYCISE3yEst6ScfMDjOz+Wb2jpktNbPfROPHmtk8Mysys8fMrGE03ijaLor2d84UqxKwiMRKqiSR9ZLBbuACdz8FOBUYYGY9gDuBe939OGAzMDI6fiSwORq/NzourUM+AY/7r7tYuXIhCxb8pWzs5JNPZNarz/Dm3BnMnjOd07ufEjDC3FZQ0J7nZkxh/sK/MG/BC/zwmhEA3PrvY1i46CXemDeDKY+Oo3nzZmEDzUUNG9B6/AO0mfgQbR75I02vHFFh9xGjf0S7F2fsd9ph5/Wm/ZxZNDjh+DoKtG65Z7+kv467u2+LNhtEiwMXAE9G45OAodH6kGibaH9fM0tbZh/yCfhPjzzJ0KHDK4yNHTuG22/7HWf3GMTYW+9h7NhfBIou9yVTSW668TbO7P41+va5hKsLv8cJ3Y5j1itzOOuMAfQ8axBFRSv5159dEzrU3LNnL5tG/yufjLiKT0ZcRaMeZ9LgpC8B0OCE47FmTfc7xQ4/nMaXXsyepe/VdbR1psQt68XMCs1sYbmlsPy1zCzPzBYDxcBLwIfAp+6ejA5ZBRRE6wXAxwDR/i1A63SxHvIJ+PXX57Np05YKY+7QLPrhPeKII1i3dn2I0GJh/boNvLN4KQDbtm1nxYoijj76KF6ZOYdUKgXAgvlvU1BwVMgwc5bv3FW6kp+P5eWV1meJBM2u/Re2jntwv+ObXX0l26dMxffsqdtA65C7VWPx8e7evdwyvuK1POXupwIdgDOBbjUZ6wHPgjCz77v7H2symPri+ut/w7Tpk7nt9htJJBJc0OeS0CHFQqdOBZx8ykksXLC4wvj3rriUp596LlBUOS6RoM2EB8krKGDHM8+y971lNL70EnbPeYOSjZsqHJp/fFcSRx7J7jfn0uS7/xwo4NpXG7Mg3P1TM5sFnA20MLP8qMrtAKyODlsNdARWmVk+0BzYmO66B1MB/6aqHeXL+mRy60HcIoyrrr6cG66/lROO78kN19/KuHEZe+mSQZMmjXnkzw8w5vpb2bp1W9n4z35+DclkksemTgsYXQ4rKeGT719N8cWX0uBL3Wh4yskc3uc8tj/1dMXjzDjiR9ew9fcPhImzDlWnBZGOmbU1sxbR+uFAf2AZMAv4VnTYcODzH97p0TbR/lfc0/86sHT7zezdqnYBx7t7o7T/AqBJ4871flZep04deOqpCZxxxtcAWLP2XY5uf3LZ/rXrltD+qK+ECi+jPKvfnaT8/HyeeOohXn55Nn+4f0LZ+Hcvv4QrrxzGN75+OTs//1O6nlpx2jGhQ8io6YgrwKDx0CFlLYa8dkeSWrOWT0b+gCMfm0LJzp2l461aUbL1MzbfcBN7V7wfMuwK2s+ZddCvoph39MVZ55yz1jxd5f3M7GRKH6rlUVqsPu7ut5hZF2Aq0Ap4G7jc3Xeb2WHAI8BpwCbgO+7+Ubr7Z2pBtAO+RulUiwqxAW9kODdnrV1bzLnn9mD27Lmcf35PPvxwZeiQctofxt3BihUfVki+/fr35rrrChk4YFi9T771VaJFczyZxLdth4YNaXTG6Wyb8ijFQ75ombV7cQYbvnM5AOsHDy0bb3X/vWz9/bh6lXxrSk1VfO7+LqXJdN/xjyjtB+87vgu4tDr3yJSAnwOauvvifXeY2avVuVF9NXHifZzbuwetW7fk/Q/eZOzYexl17Rju/u2vyM/LZ9fu3YwapVkQB6rH2d0Z9t2L+etflzPnzdI+7y2//i133f1vNGzUkGn/MxmABfMX85PRvwwZas5JtG5Ni5vGQCIBiQS7XnmV3W/MDR1WcJlaC/VJ2hZETciFFkSuq+8tiDjIhRZEHNREC+L1o76Vdc7pte7JoNla7wUhIrGSQx+KrAQsIvHi5E4LQglYRGIlmUM9YCVgEYkVVcAiIoGoBywiEogqYBGRQFQBi4gEklIFLCISRg59JqcSsIjES4kqYBGRMHLpvQ+UgEUkVvQQTkQkkJL0n4NZrygBi0ispEIHUA1KwCISK5oFISISiGZBiIgEolkQIiKBqAUhIhKIpqGJiASSUgUsIhKGKmARkUCUgEVEAsmhj4RTAhaReFEFLCISiF6KLCISSC7NA06EDkBEpCaVVGNJx8w6mtksM3vPzJaa2ehovJWZvWRmH0RfW0bjZmb3mVmRmb1rZl/NFKsSsIjESk0lYCAJ/NTdTwR6ANea2YnAGGCmu3cFZkbbAAOBrtFSCIzLdAMlYBGJFa/GkvY67mvdfVG0vhVYBhQAQ4BJ0WGTgKHR+hBgspeaC7Qws/bp7qEesIjESm30gM2sM3AaMA9o5+5ro13rgHbRegHwcbnTVkVja6mCKmARiZVUNRYzKzSzheWWwn2vZ2ZNgaeA69z9s/L73D2bYrpKtV4B707ure1biNS6Ns9OCB2CZKmkGvnQ3ccD46vab2YNKE2+U9z96Wh4vZm1d/e1UYuhOBpfDXQsd3qHaKxKqoBFJFZqcBaEAROAZe5+T7ld04Hh0fpwYFq58Sui2RA9gC3lWhWVUg9YRGKlBt+QvRfwPWCJmS2Oxm4E7gAeN7ORwN+Bb0f7ZgCDgCJgB/D9TDdQAhaRWKmplyK7+xyo8vON+lZyvAPXVuceSsAiEitJy50PJVICFpFYyZ30qwQsIjGjd0MTEQmkOtPQQlMCFpFYyZ30qwQsIjGjFoSISCCpHKqBlYBFJFZUAYuIBOKqgEVEwlAFLCISiKahiYgEkjvpVwlYRGImmUMpWAlYRGJFD+FERALRQzgRkUBUAYuIBKIKWEQkkJSrAhYRCULzgEVEAlEPWEQkEPWARUQCUQtCRCQQtSBERALRLAgRkUDUghARCUQP4UREAlEPWEQkELUgclTR+3PZum0bqVQJyWSSHmcPCh1SLCUSCebNfZ41q9cx5JvDQ4eT0y68ZDhNGjcmkUiQl5fH4w/fx09vvp2V/1gFwNZt22jWtClPTfoDe5NJfnX7f7Ls/Q9JplJcNKAvV1/xz4H/BTXP9RAud/XrfykbN24OHUas/fhHV7F8+Qcc0axZ6FBi4eH776Bli+Zl2/9x6y/K1u++/79p2qQxAC++Mps9e/fyzCPj2LlrF0Mu+wGD+p9PQft2dR5zbarJj6U3s4eBwUCxu385GmsFPAZ0BlYC33b3zWZmwO+AQcAOYIS7L0p3/USNRSqShYKC9gwa2JeHH340dCix5+688MprDOp/PgBmxs5du0gmU+zevYcGDRqUJec4KcGzXrIwERiwz9gYYKa7dwVmRtsAA4Gu0VIIjMt08YwJ2My6mVlfM2u6z/i+QeU8d+f5GY8yb+7zXDXystDhxNI9//EbxvxiLCUlufSsuv4yMwp/chPfvvJHPDFtRoV9b73zV1q3bMkxHQsA6N/nHA4/7DD6DPku/S++ghHDLqb5EfH7K8Tds16yuNZrwKZ9hocAk6L1ScDQcuOTvdRcoIWZtU93/bQtCDP7MXAtsAyYYGaj3X1atPs24IWM/4Iccl6fb7JmzTratm3NC89PZcWKImbPmRc6rNj4+qB+FBd/wqK3l3Be77NDhxMLk8f9lnZt27Bx86dcfd2NHHtMR7qf+hUAZrz0KoP6n1d27JL3VpCXSPDKtCl8tnUbw3/4M3p0P42OBWlzRM6pzkM4MyuktFr93Hh3H5/htHbuvjZaXwd83sMpAD4ud9yqaGwtVchUAV8NnO7uQ4HzgZvNbPTnsVd1kpkVmtlCM1tYUrI9wy3qjzVr1gGwYcNGpk17njPOODVwRPHSs2d3vjH4Qoren8uUPz1Anz69mDTxvtBh5bR2bdsA0LplC/r27smS91YAkEymePn/3mBA395lx8546VV69ehOg/x8Wrdswaknn8jS5R8Eibs2eXX+cx/v7t3LLZmSb8V7lZbRB9x0zpSAE+6+LbrRSkqT8EAzu4c0Cbj8PyqRaHKgsdWpxo0Pp2nTJmXr/fudx9KlKwJHFS83/fIOOnfpznHH9+Cyy69h1qzXGT7ix6HDylk7du5i+/YdZetvzF9E1y6dAZi78G26HNOBo45sW3Z8+3Ztmf/WO2XHv7t0Occe07HO465tKfeslwO0/vPWQvS1OBpfDZT/hnaIxqqUaRbEejM71d0XA7j7NjMbDDwMfOVAIq+v2rVry5NPTAAgPz+PqVOf5S8vvho2KJE0Nm7azOgbbwUglUwx6MLzOadHdwCef/n/GNjv/ArHD7v4G/zytnsYctkPcJyhgy7khOOOreuwa10dzAOeDgwH7oi+Tis3PsrMpgJnAVvKtSoqZeka0WbWAUi6+7pK9vVy99czRZrfsCB3JuWJVGHnmtmhQzgkNGjTpcq/rLN1dkGfrHPOm6tnpb2fmT1K6V/+bYD1wK+AZ4HHgU7A3ymdhrYpmob2e0pnTewAvu/uC9NdP20F7O6r0uzLmHxFROpaTb4Qw92HVbGrbyXHOqWTFrKmF2KISKzopcgiIoHozXhERAJJee68yEcJWERiRW/GIyISiHrAIiKBqAcsIhJIiVoQIiJhqAIWEQlEsyBERAJRC0JEJBC1IEREAlEFLCISiCpgEZFAUp4KHULWlIBFJFb0UmQRkUD0UmQRkUBUAYuIBKJZECIigWgWhIhIIHopsohIIOoBi4gEoh6wiEggqoBFRALRPGARkUBUAYuIBKJZECIigeghnIhIIGpBiIgEolfCiYgEogpYRCSQXOoBWy79tqgrZlbo7uNDxxFn+h7XPn2P679E6ADqqcLQARwC9D2uffoe13NKwCIigSgBi4gEogRcOfXNap++x7VP3+N6Tg/hREQCUQUsIhKIErCISCBKwOWY2QAzW2FmRWY2JnQ8cWRmD5tZsZn9NXQscWVmHc1slpm9Z2ZLzWx06JikcuoBR8wsD3gf6A+sAhYAw9z9vaCBxYyZ9Qa2AZPd/cuh44kjM2sPtHf3RWbWDHgLGKqf5fpHFfAXzgSK3P0jd98DTAWGBI4pdtz9NWBT6DjizN3XuvuiaH0rsAwoCBuVVEYJ+AsFwMfltlehH1rJcWbWGTgNmBc2EqmMErBITJlZU+Ap4Dp3/yx0PLI/JeAvrAY6ltvuEI2J5Bwza0Bp8p3i7k+HjkcqpwT8hQVAVzM71swaAt8BpgeOSaTazMyACcAyd78ndDxSNSXgiLsngVHAXyh9aPG4uy8NG1X8mNmjwJvACWa2ysxGho4phnoB3wMuMLPF0TIodFCyP01DExEJRBWwiEggSsAiIoEoAYuIBKIELCISiBKwiEggSsAiIoEoAYuIBPL/Y7gsT5Z1h1MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "yV2NNFA0Wtif",
        "outputId": "db84d4f7-dda3-4779-8948-006e228f4230"
      },
      "source": [
        "# predict recipient gender and print prediction results\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "print_results(rf_clf, y_pred)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score ->  51.927616050354054\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.27      0.14      0.18       300\n",
            "         1.0       0.49      0.32      0.38       384\n",
            "         2.0       0.57      0.85      0.68       587\n",
            "\n",
            "    accuracy                           0.52      1271\n",
            "   macro avg       0.44      0.43      0.42      1271\n",
            "weighted avg       0.47      0.52      0.47      1271\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD4CAYAAADSIzzWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZDUlEQVR4nO3deXwV5b3H8c8vEPawqRdiQAGhuPTeauUq4FJ2CMgFAXEXFIzKZosbuFwu1VrArdJebWlFwSJWFgU3lFVF2WRxYbkFLUgQRBAIS4DknOf+kRGChuREcvLkDN+3r3ll5pnJzI8j/M7vPPPMc8w5h4iIlL4k3wGIiJyslIBFRDxRAhYR8UQJWETEEyVgERFPysf9AhXSNMwizuqnnOo7hNDrnXKu7xBOCqM3TrYTPUfOji9jzjnJpzY64eudCFXAIiKexL0CFhEpVdGI7whipgQsIuESyfUdQcyUgEUkVJyL+g4hZkrAIhIuUSVgERE/VAGLiHiim3AiIp6oAhYR8cNpFISIiCe6CSci4om6IEREPNFNOBERT1QBi4h4optwIiKe6CaciIgfzqkPWETED/UBi4h4oi4IERFPVAGLiHgSyfEdQcyUgEUkXNQFISLiibogREQ8UQUsIuKJErCIiB9ON+FERDxRH7CIiCfqghAR8UQVsIiIJ6qARUQ8UQUsIuJJriZkTyhJSUksWfw2X2/ZRrcr+zDgjr4MGdyfxo0bUif15+zcuct3iAmtUeMz+ePfxhzZrt+gHk/9/hkWL1zGI088SJWqVdjy1df8+vbh7Nu732OkiaXXmNs4p80F7NuZxVMd7wUg9dwz6fG7fpSvmEw0N8qrD40n85MvAPivEX1o2vp8crIP88rdz/L16o0eo4+jBKqAk3wHUBYMGdyfdevWH9n+aNEyOqZfw8aNmz1GFR5fbthEl1ZX06XV1XRtcy0HDxzk3Tfn8funRzDmt0+Tflkv3nlzHhmD+voONaEsn/oez/UZdUxb52HXMefpaTzdeTjvPjmFzsOvA6Bpq/M5tWFdHmv1G6bf/1eu/F0/HyGXjmg09sWzkz4Bp6Wl0jm9LePHTz7StmrVajZtyvQYVXhdcvnFbNq4mS2ZW2l41pks+Wg5AAsXLKJT17aeo0ss/1q6juw9+37Q6qhYrTIAlapXIeubvE9v53W4kOXTPwDgq5UbqJxShZTTapZmuKXHRWNfPCuyC8LMzga6AWlB0xZgpnNubTwDKy1PPjGSYcMfISWlmu9QTgpX9OjE69NnAbB+3Re079ya2W/Np3O3DqSm1fUcXeJ7feRE+k0cTpf7b8CSjGd6jgCgep3a7Pl655Hj9mz7jup1a7P3292+Qo2fMlDZxqrQCtjM7gNeBgxYGiwGTDazYfEPL766dG7H9u07WLHyM9+hnBSSk8vTrtOveGvGuwDcO2QEN95yNTPnTqZqtSrkHE6cR0jLquY3tOf1h1/k9y0H8cbDL9JrdIbvkEpfiCrgfsB5zrlj/mWY2ZPAamBUQb9kZhlABoCVq0FSUtUSCLXktWzZjK5XdCC9UxsqVapI9eopTHhhLH36DvEdWii1ancpqz9dx45vvwPgy/UbuanX7QA0POtM2nS43Gd4oXBhz8uZOXICAJ++uZieo24FIOub76hx+ilHjqtRtzZZ277zEmPcJdAoiKL6gKPA6QW0pwb7CuScG+eca+aca1ZWky/AAw+OokGjZjT+WXOuv2EA8+d/qOQbR117pDNz+ttHtk85tTYAZsagu25l0vNTfIUWGlnbd9Go+TkAnNXyPHZs3AbAmtkruLDHZQCccUFjDu49EM7uBwDnYl88K6oC/jUw18zWA98PCTgDaAwMimdgPg0aeAt33zWAunVPY+XyObw9ax633X6P77ASWuUqlbm0VXMeGPrwkbauPTpxU79rAJj15lymvPSar/AS0rVjB9Oo+TlUrZXC/Yv+xOynpjJt2F/pOuImksqXI/dQDtOH/w2AdfNX0rT1+dz73h84nH2IKff8xXP0cZRAfcDmingXMLMk4CKOvQm3zDkXieUC5Suk+X+bCbn6Kaf6DiH0eqec6zuEk8LojZPtRM+RPemhmHNO5esfPuHrnYgiR0E456LA4lKIRUTkxJWBm2ux0pNwIhIukZg+nJcJSsAiEi4J1Ad80j8JJyIhU8KPIptZOTNbaWZvBNsNzWyJmW0ws3+YWYWgvWKwvSHY36CocysBi0i4lPyDGHcC+Z/8HQ085ZxrDOwi73kJgp+7gvanguMKpQQsIqHioi7mpShmVg/oAvwt2DagDTA1OGQC0D1Y7xZsE+xvGxx/XErAIhIuxeiCMLMMM/s43/LDZ7f/ANzL0QfPTgF2O+e+f9wuk6NDdNMInpcI9u8Jjj8u3YQTkXApxigI59w4YFxB+8zsCmC7c265mbUqmeCOpQQsIuFScqMgLgH+y8w6A5WA6sDTQE0zKx9UufXIeziN4Gd9INPMygM1gJ0/Pu1R6oIQkXApoVEQzrnhzrl6zrkGwDXAPOfc9cB8oFdwWB9gRrA+M9gm2D/PFfGosRKwiIRL/CfjuQ8YamYbyOvjfS5ofw44JWgfChQ5Za+6IEQkXOLwIIZzbgGwIFj/krz5cX54zEHgquKcVwlYRMIlhuFlZYUSsIiEi+aCEBHxwyXQXBBKwCISLuqCEBHxRPMBi4h4ogpYRMSTXN2EExHxQ10QIiKeqAtCRMQPDUMTEfFFFbCIiCdKwCIinuhRZBERP2L5rreyQglYRMJFCVhExBONghAR8UQVsIiIJ0rAIiJ+uIi6II6oU7VmvC9x0mtUuY7vEEKvmtP31yYMVcAiIn5oGJqIiC9KwCIiniROF7ASsIiEi8tNnAysBCwi4ZI4+VcJWETCRTfhRER8UQUsIuKHKmAREV9UAYuI+OFyfUcQOyVgEQmVBPpWeiVgEQkZJWARET9UAYuIeKIELCLiiYuY7xBipgQsIqGiClhExBMXVQUsIuJFIlXA+p4VEQkV5yzmpTBmVsnMlprZJ2a22sxGBu0NzWyJmW0ws3+YWYWgvWKwvSHY36CoWJWARSRUXDT2pQiHgDbOuV8A5wOdzKw5MBp4yjnXGNgF9AuO7wfsCtqfCo4rlBKwiIRKNGIxL4VxefYFm8nB4oA2wNSgfQLQPVjvFmwT7G9rZoVeRAlYRELFRS3mpShmVs7MVgHbgdnAF8Bu547MOJEJpAXracBmgGD/HuCUws6vBCwioVKcBGxmGWb2cb4l45hzORdxzp0P1AMuAs4uyVg1CkJEQsUVYzpg59w4YFwMx+02s/lAC6CmmZUPqtx6wJbgsC1AfSDTzMoDNYCdhZ1XFbCIhEpJdUGY2WlmVjNYrwy0B9YC84FewWF9gBnB+sxgm2D/POcKfztQBSwioVLU8LJiSAUmmFk58orVV5xzb5jZGuBlM3sEWAk8Fxz/HPCimW0AvgOuKeoCSsAiEiqREpoLwjn3KXBBAe1fktcf/MP2g8BVxbmGErCIhEoJVsBxpwQsIqGiuSBERDwpzigI35SARSRUVAGLiHgSiSbO6NqTOgGf1bgBz45/4sj2GWfW4/Hf/4m6p/8b7Tu24nBODpv+tZmhAx8kK2uvx0gTz92PD6V5u+bs3rGb/u3yHi7KePBWWrRrTm5ODl9v2sqYoY+zP2s/1WumMGLcQzT9RVPemfIuf3zwfz1Hnxi6PnYrTdpcwP6dWfylwzAA6pxzBp0fvYUKVSqxO/NbXr3zGQ7vy6ZyzWr0+vOdnP4fjfhk6vvM+u8JRZw9cSVSF0TivFXEwRcbNtLh8p50uLwnnVpdRXb2Qd5+cw7vz19Em5bdaX9pD778YhODht7qO9SE886U2Qy/4f5j2pa/v4J+bW/l1va3k/llJtcNyhsmefhQDs8/NoE/P1zkA0mSzydTPuClPmOOabtidH/mjnqZv3Qcxrp3PqblbV0AyD2Uw4LHpzD7dy/5CLVURZ3FvPh2Uifg/C79VXM2bdzMls1beX/+R0QiEQBWLPuE1NPreI4u8Xy25DOydh/7qWH5+8uJRvLmAFyzYh2npp4GwMHsg3y+bDU5hw6XepyJ7Kul68jeve+YttoNU/lqyToA/vXBZ5ydnjdcNSf7EJs//ie5h3JKPc7SVlLzAZeGn5yAzezmkgzEt2490nlt2ls/ar/mhh7Mn/OBh4jCLf3qjiybv8x3GKHz7fpMmna4EIBzulxM9dTaniMqfc7Fvvh2IhXwyOPtyD/D0P5Du07gEqUjOTmZDumteeO1d45pH3JXBrm5uUx/5Q1PkYXTdYOvJRKJMGf6XN+hhM7r94zjwhvb0/+NR6hYtTKRnNyifylkEqkLotCbcGb26fF2Acf9XJ5/hqG0WueVgfeZwrVudymffbKGHd8enbio97XdadfhV/Tu3q+Q35Ti6nhVe1q0u5i7r77PdyihtPOLrbx04ygAajesS+M253uOqPSFaRREHaAjeV+7kZ8BH8UlIg+69+p8TPdDq7aXcseQW+h5RR8OZh/0GFm4/GerZlx9R29+0+tuDh085DucUKpySnUO7MwCMy4b3J3lk06+TxllvuLLxwqbLc3MngOed84tLGDfS86564q6QFmvgCtXqcyyz+bQ4oKO7M3Ku6GxcPnbVKyYzK7v9gCw4uNPGDb0tz7DLNTZVdOKPqiUPfCn4fyixX9Qo3YNdu3YxYQnXuTaQVeTXKECWbuyAFi7Yi1/GD4WgEmLJlIlpQrJycnsy9rHfdcNZ9P6r3z+EY7RqtxpvkP4kSvHDuTMFudQpVYK+3dk8d5TU6lQpRLNbmoPwLpZy5g3+h9Hjh+88A9UTKlMueTyHMw6wKQbR7Fj/Zbjnd6LhzZNOuF+gY9Se8acc1punea1H6LQBFwSynoCDoOymIDDpiwm4DAqiQT8Yd1eMeecS7ZN9ZqAT+oHMUQkfIr+suOyQwlYRELF4X90Q6yUgEUkVHLLwPCyWCkBi0ioqAIWEfFEfcAiIp6oAhYR8UQVsIiIJxFVwCIifiTQNxIpAYtIuERVAYuI+JFIcx8oAYtIqOgmnIiIJ1FTF4SIiBcR3wEUgxKwiISKRkGIiHiiURAiIp5oFISIiCfqghAR8UTD0EREPImoAhYR8UMVsIiIJ0rAIiKeJNBXwikBi0i4qAIWEfEkkR5FTvIdgIhISYpa7EthzKy+mc03szVmttrM7gzaa5vZbDNbH/ysFbSbmY01sw1m9qmZ/bKoWJWARSRUosVYipAL3OWcOxdoDgw0s3OBYcBc51wTYG6wDZAONAmWDODZoi6gBCwioVJSCdg5t9U5tyJY3wusBdKAbsCE4LAJQPdgvRsw0eVZDNQ0s9TCrqEELCKh4oqxmFmGmX2cb8ko6Jxm1gC4AFgC1HHObQ12bQPqBOtpwOZ8v5YZtB2XbsKJSKgUZy4I59w4YFxhx5hZNWAa8GvnXJblm/DdOefM7CfP/6MELCKhUpKjIMwsmbzkO8k5Nz1o/sbMUp1zW4Muhu1B+xagfr5frxe0HVfcE/A3+3fH+xInvR3ZWb5DCL1ZmX/2HYLEKFpCE1JaXqn7HLDWOfdkvl0zgT7AqODnjHztg8zsZeBiYE++rooCqQIWkVApwQcxLgFuBD4zs1VB2/3kJd5XzKwfsAnoHex7C+gMbAAOADcXdQElYBEJlZKakN05txCO+/UabQs43gEDi3MNJWARCRU9iiwi4knuTx+UUOqUgEUkVBIn/SoBi0jIqAtCRMSTkhqGVhqUgEUkVBIn/SoBi0jIqAtCRMSTSALVwErAIhIqqoBFRDxxqoBFRPxQBSwi4omGoYmIeJI46VcJWERCJjeBUrASsIiEim7CiYh4optwIiKeqAIWEfFEFbCIiCcRpwpYRMQLjQMWEfFEfcAiIp6oD1hExBN1QYiIeKIuCBERTzQKQkTEE3VBiIh4optwIiKeqA9YRMSTROqCSPIdQFmQlJTEsqXvMOPVCQAMuKMv69YsJPfwFk45pZbn6MIhKSmJJYvf5tXpzwPQuvUlLF70FkuXzGLevGmc1aiB3wATVCQSoVffgQy4ZwQAS5av4qqbB9H9htu5/+HHyc2NALB3334G3juCHn0G0O3623j1zXd9hh1XzrmYF9+UgIEhg/uzbt36I9sfLVpGx/Rr2Lhxs8eowmXwoH6s+78NR7b/OPZR+vYdwkUXd+IfL89g2PAhHqNLXH+fMoNGDc4AIBqNcv8jT/DYyGG89vc/c3rdf2PG23MAmDztdc5qcAbTJzzD838azWN//Cs5OTk+Q4+bCC7mxbeTPgGnpaXSOb0t48dPPtK2atVqNm3K9BhVuKSl1SU9vQ3PP3/0NXbOkVK9GgDVa6Swdes3vsJLWNu2f8v7Hy2lZ9eOAOzek0Vy+fI0OKMeAC3+85fMWbAQADNj/4FsnHMcyD5IjeoplCtXzlvs8RTFxbz4VmQfsJmdDaQBS5xz+/K1d3LOzYpncKXhySdGMmz4I6SkVPMdSmg9/tj/MPz+R0lJqXqk7fY77mXGaxPJzj7I3r17uezybh4jTEyjn/4LQwf0Y/+BbABq1axBJBLl87X/5Ofn/Ix3Fyxk2/YdAFzXsyuD7htJ627Xs/9ANo//djhJSeGsv8pC10KsCv0/YGZDgBnAYOBzM8v/r+TReAZWGrp0bsf27TtYsfIz36GEVuf0tnz77U5W/uA1HjK4P92638RZjS9i4sRXGDPmvz1FmJgWfLiE2rVqct7ZTY60mRmP/XYYY8aO45r+d1K1SuUjSfbDpcs5u0kj5s+YxLQX/pdHn3yGffv3+wo/rsJUAd8KXOic22dmDYCpZtbAOfc0YMf7JTPLADIArFwNkpKqHu9Qr1q2bEbXKzqQ3qkNlSpVpHr1FCa8MJY+fdUfWVJatGxGly7t6dipNZUq5r3Gr736Ak2bNmbZslUATJn6Oq/PfNFzpIll5adrWLBwMR8sWsahwzns33+A+0aOYfSIe5n47OMAfLhkOZs2bwHg1Tdn0/+G3pgZZ9Q7nbTUuvxrUyb/fm5Tn3+MuEikYWhFfQZJ+r7bwTm3EWgFpJvZkxSSgJ1z45xzzZxzzcpq8gV44MFRNGjUjMY/a871Nwxg/vwPlXxL2EMPjeasxhfRtGlLbrxpIAsWfEjPXv2oXj2FJo0bAtC27WWsW7ehiDNJfr+542bmvvZ33p02gcdGDuOiC3/B6BH3snPXbgAOHz7M+ElT6N29MwCpdU5j8fK8N7wd3+1i41eZ1Du9rrf44yniXMyLb0VVwN+Y2fnOuVUAQSV8BTAe+Pe4R+fJoIG3cPddA6hb9zRWLp/D27Pmcdvt9/gOKzQikQh3DLiPl18eRzQaZdfuPdx2292+wwqF5ydN5b2PluKiUa6+sgsXX3g+ALf3vY4HfvcEV954B845fjPgFmrVrOE52vgoC10LsbLCOqzNrB6Q65zbVsC+S5xzHxZ1gfIV0hLn1UhQ5UJ6M6Us2Zf5nu8QTgrJpzY67ifrWLVIax1zzlm0Zf4JX+9EFFoBO+eOOxYrluQrIlLaQjMKQkQk0ZTkKAgzG29m283s83xttc1stpmtD37WCtrNzMaa2QYz+9TMflnU+ZWARSRUXDH+i8ELQKcftA0D5jrnmgBzg22AdKBJsGQAzxZ1ciVgEQmViIvGvBTFOfc+8N0PmrsBE4L1CUD3fO0TXZ7FQE0zSy3s/ErAIhIqxZmMx8wyzOzjfEtGDJeo45zbGqxvA+oE62lA/glkMoO249J0lCISKsUZhuacGweM+6nXcs45M/vJd/1UAYtIqJRwH3BBvvm+ayH4uT1o3wLUz3dcvaDtuJSARSRUos7FvPxEM4E+wXof8ubL+b79pmA0RHNgT76uigKpC0JEQqUk54Iws8nkTcFwqpllAiOAUcArZtYP2AT0Dg5/C+gMbAAOADcXdX4lYBEJlVhGN8TKOXftcXa1LeBYBwwszvmVgEUkVE6ga6HUKQGLSKgk0nSUSsAiEiqqgEVEPFEFLCLiScRFfIcQMyVgEQmVRJqOUglYREIlkb4RQwlYREJFFbCIiCcaBSEi4olGQYiIeFKSjyLHmxKwiISK+oBFRDxRH7CIiCeqgEVEPNE4YBERT1QBi4h4olEQIiKe6CaciIgn6oIQEfFET8KJiHiiClhExJNE6gO2RHq3KC1mluGcG+c7jjDTaxx/eo3LviTfAZRRGb4DOAnoNY4/vcZlnBKwiIgnSsAiIp4oARdM/Wbxp9c4/vQal3G6CSci4okqYBERT5SARUQ8UQLOx8w6mdn/mdkGMxvmO54wMrPxZrbdzD73HUtYmVl9M5tvZmvMbLWZ3ek7JimY+oADZlYO+CfQHsgElgHXOufWeA0sZMzscmAfMNE593Pf8YSRmaUCqc65FWaWAiwHuuvvctmjCvioi4ANzrkvnXOHgZeBbp5jCh3n3PvAd77jCDPn3Fbn3IpgfS+wFkjzG5UURAn4qDRgc77tTPSXVhKcmTUALgCW+I1ECqIELBJSZlYNmAb82jmX5Tse+TEl4KO2APXzbdcL2kQSjpklk5d8JznnpvuORwqmBHzUMqCJmTU0swrANcBMzzGJFJuZGfAcsNY596TveOT4lIADzrlcYBDwDnk3LV5xzq32G1X4mNlkYBHQ1Mwyzayf75hC6BLgRqCNma0Kls6+g5If0zA0ERFPVAGLiHiiBCwi4okSsIiIJ0rAIiKeKAGLiHiiBCwi4okSsIiIJ/8PG0DY3bA0SDsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "YJc2mgUVWwS1",
        "outputId": "e0d0f6da-e9b2-4ed7-df93-c533ee4e6589"
      },
      "source": [
        "# predict recipient gender and print prediction results\n",
        "y_pred = mlp_clf.predict(X_test)\n",
        "print_results(mlp_clf, y_pred)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score ->  50.511408339889854\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.32      0.30      0.31       300\n",
            "         1.0       0.45      0.36      0.40       384\n",
            "         2.0       0.61      0.70      0.65       587\n",
            "\n",
            "    accuracy                           0.51      1271\n",
            "   macro avg       0.46      0.45      0.45      1271\n",
            "weighted avg       0.49      0.51      0.50      1271\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "VTGvXpeAWybS",
        "outputId": "8fafbc43-24e1-4a52-f3ce-b047e02cf536"
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# predict recipient gender and print prediction results\n",
        "y_pred = clf.predict(X_test)\n",
        "print_results(clf, y_pred)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/dummy.py:132: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.\n",
            "  \"stratified to prior in 0.24.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score ->  34.93312352478363\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.24      0.28      0.26       300\n",
            "         1.0       0.29      0.28      0.28       384\n",
            "         2.0       0.46      0.43      0.44       587\n",
            "\n",
            "    accuracy                           0.35      1271\n",
            "   macro avg       0.33      0.33      0.33      1271\n",
            "weighted avg       0.36      0.35      0.35      1271\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}