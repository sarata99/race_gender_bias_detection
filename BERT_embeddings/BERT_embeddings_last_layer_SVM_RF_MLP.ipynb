{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "OFFICIAL BERT_embeddings_last_layer_SVM_RF_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a4ee89f245b45b9b89e7084e1e15c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8a24b670bc50484dbf9ee82c6ac200ff",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6b050fdc3e3b41fdb1ee2896fe81ded1",
              "IPY_MODEL_f9b0a8d5dcd947c6bf762f5613426de2"
            ]
          }
        },
        "8a24b670bc50484dbf9ee82c6ac200ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b050fdc3e3b41fdb1ee2896fe81ded1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e0a8afbf3aa74d4c9b11d77bb4e667e7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_432fe627d7c14c10b3dc5691e1543603"
          }
        },
        "f9b0a8d5dcd947c6bf762f5613426de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b67817ec0925461fa1557a2ed6aa64df",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 241kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_230cf49b5a06424cbe3ec028aa14cc81"
          }
        },
        "e0a8afbf3aa74d4c9b11d77bb4e667e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "432fe627d7c14c10b3dc5691e1543603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b67817ec0925461fa1557a2ed6aa64df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "230cf49b5a06424cbe3ec028aa14cc81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c68956b1a9f4b45a62be847bf0ab8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_32cf2ea747c9436a854104b11f875bed",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_66e771438f664a1fb978785c64629a42",
              "IPY_MODEL_6aaeb5c58bb944b8aef54fe7a93c9eb4"
            ]
          }
        },
        "32cf2ea747c9436a854104b11f875bed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66e771438f664a1fb978785c64629a42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_eca35f14eb844b7587e8613f36b14ca0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b3ea06d9771493ba06145535d3c3265"
          }
        },
        "6aaeb5c58bb944b8aef54fe7a93c9eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d6921c05ff8e4a39a67478f52d385540",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 108B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1ceb87b5476b43648ba64343158c8fff"
          }
        },
        "eca35f14eb844b7587e8613f36b14ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b3ea06d9771493ba06145535d3c3265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6921c05ff8e4a39a67478f52d385540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1ceb87b5476b43648ba64343158c8fff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f2675c5bfc654ce79fbfd545dbde8ad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9114630eabc1482ca5c75ee6f56ae278",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_13353567d8a849f4b0a5578d10450991",
              "IPY_MODEL_42db57641e444dd598e0215c2c25cb45"
            ]
          }
        },
        "9114630eabc1482ca5c75ee6f56ae278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "13353567d8a849f4b0a5578d10450991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_92b66ff56c0d494ca01c48d0dc2591fc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2be90d6c591643f5b6983c64994e914f"
          }
        },
        "42db57641e444dd598e0215c2c25cb45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_77c25e2c9d3e427d9a0c5ddc6ea663c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 2.61MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_791343cd4e3049039a7c8c8ab1a2f266"
          }
        },
        "92b66ff56c0d494ca01c48d0dc2591fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2be90d6c591643f5b6983c64994e914f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77c25e2c9d3e427d9a0c5ddc6ea663c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "791343cd4e3049039a7c8c8ab1a2f266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31E5M9tLwwN2",
        "outputId": "26a3c095-feeb-4ea7-9a79-e7476c861eef"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "\n",
        "import torch\n",
        "!pip install transformers\n",
        "import transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 21.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 21.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5kPynH3wyf7"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "AS59orfow2cQ",
        "outputId": "8f8350c8-6ddc-469e-9b58-4f7327b3c74c"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1fff25ed-a273-4a37-95cd-93a8bb63982e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1fff25ed-a273-4a37-95cd-93a8bb63982e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving FA18_sentences.xlsx to FA18_sentences.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "cQdFvrUpw25z",
        "outputId": "af30441a-242e-46d8-cfc3-428916019c39"
      },
      "source": [
        "df_fall_18_messages = pd.read_excel('FA18_sentences.xlsx', index_col=0)\n",
        "print(df_fall_18_messages.shape)\n",
        "df_fall_18_messages.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6354, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Recipient Gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hey @Katie Poteet I know you said we should em...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iâ€™m planning to do the Baltimore Community Too...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@Mary Cassell I would email Dr. K anyway with ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If not, itâ€™s still good for her to know what y...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ok, thanks a lot.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence  Recipient Gender\n",
              "0  Hey @Katie Poteet I know you said we should em...                 1\n",
              "1  Iâ€™m planning to do the Baltimore Community Too...                 1\n",
              "2  @Mary Cassell I would email Dr. K anyway with ...                 1\n",
              "3  If not, itâ€™s still good for her to know what y...                 1\n",
              "4                                  Ok, thanks a lot.                 1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWK6MFqLxAEP"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df_fall_18_messages['Sentence'].values\n",
        "labels = df_fall_18_messages['Recipient Gender'].values"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOgB4WtsxFsg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "5a4ee89f245b45b9b89e7084e1e15c15",
            "8a24b670bc50484dbf9ee82c6ac200ff",
            "6b050fdc3e3b41fdb1ee2896fe81ded1",
            "f9b0a8d5dcd947c6bf762f5613426de2",
            "e0a8afbf3aa74d4c9b11d77bb4e667e7",
            "432fe627d7c14c10b3dc5691e1543603",
            "b67817ec0925461fa1557a2ed6aa64df",
            "230cf49b5a06424cbe3ec028aa14cc81",
            "1c68956b1a9f4b45a62be847bf0ab8cd",
            "32cf2ea747c9436a854104b11f875bed",
            "66e771438f664a1fb978785c64629a42",
            "6aaeb5c58bb944b8aef54fe7a93c9eb4",
            "eca35f14eb844b7587e8613f36b14ca0",
            "6b3ea06d9771493ba06145535d3c3265",
            "d6921c05ff8e4a39a67478f52d385540",
            "1ceb87b5476b43648ba64343158c8fff",
            "f2675c5bfc654ce79fbfd545dbde8ad3",
            "9114630eabc1482ca5c75ee6f56ae278",
            "13353567d8a849f4b0a5578d10450991",
            "42db57641e444dd598e0215c2c25cb45",
            "92b66ff56c0d494ca01c48d0dc2591fc",
            "2be90d6c591643f5b6983c64994e914f",
            "77c25e2c9d3e427d9a0c5ddc6ea663c7",
            "791343cd4e3049039a7c8c8ab1a2f266"
          ]
        },
        "outputId": "dfcec3aa-6dfd-49a9-976d-b037fb72cbb5"
      },
      "source": [
        "# Initialize the tokenizer with a pretrained model\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a4ee89f245b45b9b89e7084e1e15c15",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c68956b1a9f4b45a62be847bf0ab8cd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_wâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2675c5bfc654ce79fbfd545dbde8ad3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9jAYeLFxIbi",
        "outputId": "516a7f6e-2cd2-4a75-9ac5-140d0244cdde"
      },
      "source": [
        "# Convert the string \"granola bars\" to tokenized vocabulary IDs\n",
        "sentence_ids = df_fall_18_messages['Sentence'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "print('Original: ',sentences[0])\n",
        "print('Ids: ',sentence_ids[0])\n",
        "print('Tokens: ',tokenizer.convert_ids_to_tokens(sentence_ids[0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Hey @Katie Poteet I know you said we should email doctor Kaye with the volunteer opportunity we wanted but what if you donâ€™t have a specific date you want to do yet?\n",
            "Ids:  [101, 4931, 1030, 9734, 8962, 15558, 1045, 2113, 2017, 2056, 2057, 2323, 10373, 3460, 23686, 2007, 1996, 6951, 4495, 2057, 2359, 2021, 2054, 2065, 2017, 2123, 1521, 1056, 2031, 1037, 3563, 3058, 2017, 2215, 2000, 2079, 2664, 1029, 102]\n",
            "Tokens:  ['[CLS]', 'hey', '@', 'katie', 'pot', '##eet', 'i', 'know', 'you', 'said', 'we', 'should', 'email', 'doctor', 'kaye', 'with', 'the', 'volunteer', 'opportunity', 'we', 'wanted', 'but', 'what', 'if', 'you', 'don', 'â€™', 't', 'have', 'a', 'specific', 'date', 'you', 'want', 'to', 'do', 'yet', '?', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOE0v79WxPMD",
        "outputId": "6571f59f-d54b-421f-b0d4-a0605f0d133c"
      },
      "source": [
        "max_len = 0\n",
        "for id_vector in sentence_ids.values:\n",
        "    if len(id_vector) > max_len:\n",
        "        max_len = len(id_vector)\n",
        "\n",
        "padded_ids = numpy.array([i + [0]*(max_len-len(i)) for i in sentence_ids.values])\n",
        "print(\"max id array length: \", max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max id array length:  105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85UvmLLIxR1B",
        "outputId": "60ee19ff-c588-4e60-a00d-0062ed73ea2e"
      },
      "source": [
        "attention_mask = numpy.where(padded_ids != 0, 1, 0)\n",
        "print(attention_mask.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6354, 105)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi9xnOhkxTin"
      },
      "source": [
        "# Convert lists to tensors\n",
        "ids_tensor = torch.LongTensor(padded_ids)\n",
        "attention_masks_tensor = torch.tensor(attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JwCTH-ZxVwd"
      },
      "source": [
        "ids_batches = torch.tensor_split(ids_tensor, 100)\n",
        "attention_masks_batches = torch.tensor_split(attention_masks_tensor, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyk7_n1VxWlx",
        "outputId": "222dda5a-738f-48a3-f81d-067fc02c1738"
      },
      "source": [
        "model = transformers.BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "# Set the device to GPU (cuda) if available, otherwise stick with CPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImVby8a6xpEN"
      },
      "source": [
        "'''\n",
        "model_output: BaseModelOutputWithPoolingAndCrossAttentions\n",
        "      last_hidden_state (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)) â€“ Sequence of hidden-states at the \n",
        "      output of the last layer of the model.\n",
        "\n",
        "      pooler_output (torch.FloatTensor of shape (batch_size, hidden_size)) â€“ Last layer hidden-state of the first token of the \n",
        "      sequence (classification token) further processed by a Linear layer and a Tanh activation function. \n",
        "      The Linear layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n",
        "\n",
        "      hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when \n",
        "      config.output_hidden_states=True) â€“ Tuple of torch.FloatTensor (one for the output of the embeddings + one for the output \n",
        "      of each layer) of shape (batch_size, sequence_length, hidden_size).Hidden-states of the model at the output of each \n",
        "      layer plus the initial embedding outputs.\n",
        "\n",
        "https://huggingface.co/transformers/main_classes/output.html\n",
        "'''\n",
        "\n",
        "embeddings = []\n",
        "for i in range(0,len(ids_batches)):\n",
        "  with torch.no_grad():\n",
        "    ids_batch = ids_batches[i].to(device)\n",
        "    attention_masks_batch = attention_masks_batches[i].to(device)\n",
        "    model_output = model(ids_batch, attention_mask=attention_masks_batch)\n",
        "    # Get last hidden state from tuple returned from model\n",
        "    last_hidden_state = model_output[0].cpu()\n",
        "    \n",
        "    # For each sentence in a batch\n",
        "    for sentence in last_hidden_state: \n",
        "      # Get emmbedding for first token ([CLS])\n",
        "      embeddings.append(sentence[0])\n",
        "    \n",
        "    ids_batch.cpu()\n",
        "    attention_masks_batch.cpu()\n",
        "    del model_output, ids_batch, attention_masks_batch\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96bIfZmeyLT7",
        "outputId": "1550da16-1c7d-4501-c636-0a9e73c5fe8b"
      },
      "source": [
        "print(\"len(embeddings): \",len(embeddings))\n",
        "print(\"embeddings[0].shape: \",embeddings[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(embeddings):  6354\n",
            "embeddings[0].shape:  torch.Size([768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgOM4yH2yTC-"
      },
      "source": [
        "numpy_array = []\n",
        "for embedding in embeddings:\n",
        "  numpy_array.append(embedding.numpy())\n",
        "\n",
        "X = numpy.array(numpy_array)\n",
        "y = df_fall_18_messages[\"Recipient Gender\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "6BfahdGkybIA",
        "outputId": "6536dd13-b378-4663-8d54-e4eac7d65e87"
      },
      "source": [
        "df_features = pd.DataFrame(X)\n",
        "df_features.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3154</th>\n",
              "      <td>0.279578</td>\n",
              "      <td>-0.153533</td>\n",
              "      <td>-0.234437</td>\n",
              "      <td>0.054164</td>\n",
              "      <td>0.057772</td>\n",
              "      <td>-0.149382</td>\n",
              "      <td>0.532575</td>\n",
              "      <td>0.583567</td>\n",
              "      <td>-0.305407</td>\n",
              "      <td>-0.193732</td>\n",
              "      <td>-0.006841</td>\n",
              "      <td>-0.633726</td>\n",
              "      <td>-0.204653</td>\n",
              "      <td>0.163323</td>\n",
              "      <td>0.091511</td>\n",
              "      <td>0.165980</td>\n",
              "      <td>-0.029230</td>\n",
              "      <td>0.622729</td>\n",
              "      <td>0.363196</td>\n",
              "      <td>0.446545</td>\n",
              "      <td>0.149097</td>\n",
              "      <td>-0.121890</td>\n",
              "      <td>-0.095191</td>\n",
              "      <td>-0.021744</td>\n",
              "      <td>-0.097931</td>\n",
              "      <td>-0.275602</td>\n",
              "      <td>-0.068888</td>\n",
              "      <td>0.037615</td>\n",
              "      <td>-0.155489</td>\n",
              "      <td>-0.069284</td>\n",
              "      <td>0.195589</td>\n",
              "      <td>-0.032312</td>\n",
              "      <td>-0.227916</td>\n",
              "      <td>-0.252238</td>\n",
              "      <td>0.282812</td>\n",
              "      <td>0.043574</td>\n",
              "      <td>0.079997</td>\n",
              "      <td>0.056487</td>\n",
              "      <td>-0.051637</td>\n",
              "      <td>0.200767</td>\n",
              "      <td>...</td>\n",
              "      <td>0.062972</td>\n",
              "      <td>-0.508332</td>\n",
              "      <td>-0.247837</td>\n",
              "      <td>0.091102</td>\n",
              "      <td>-0.000801</td>\n",
              "      <td>0.026632</td>\n",
              "      <td>-0.160224</td>\n",
              "      <td>-0.117408</td>\n",
              "      <td>0.099556</td>\n",
              "      <td>-0.134289</td>\n",
              "      <td>-0.349096</td>\n",
              "      <td>0.352174</td>\n",
              "      <td>0.278086</td>\n",
              "      <td>-0.338788</td>\n",
              "      <td>-0.356639</td>\n",
              "      <td>0.405804</td>\n",
              "      <td>0.571006</td>\n",
              "      <td>0.098282</td>\n",
              "      <td>0.023855</td>\n",
              "      <td>-0.664829</td>\n",
              "      <td>0.266952</td>\n",
              "      <td>-0.256381</td>\n",
              "      <td>0.345070</td>\n",
              "      <td>0.407927</td>\n",
              "      <td>-7.528857</td>\n",
              "      <td>0.368115</td>\n",
              "      <td>0.070187</td>\n",
              "      <td>-0.597815</td>\n",
              "      <td>0.266644</td>\n",
              "      <td>-0.399936</td>\n",
              "      <td>-0.070742</td>\n",
              "      <td>-0.401815</td>\n",
              "      <td>0.358518</td>\n",
              "      <td>-0.464408</td>\n",
              "      <td>0.316487</td>\n",
              "      <td>-0.038343</td>\n",
              "      <td>-0.154444</td>\n",
              "      <td>-0.540360</td>\n",
              "      <td>0.218350</td>\n",
              "      <td>0.369504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2892</th>\n",
              "      <td>-0.139322</td>\n",
              "      <td>0.050318</td>\n",
              "      <td>-0.050363</td>\n",
              "      <td>-0.162064</td>\n",
              "      <td>-0.446361</td>\n",
              "      <td>-0.575043</td>\n",
              "      <td>0.475383</td>\n",
              "      <td>0.549649</td>\n",
              "      <td>0.093238</td>\n",
              "      <td>-0.400455</td>\n",
              "      <td>-0.354006</td>\n",
              "      <td>-0.083817</td>\n",
              "      <td>-0.314904</td>\n",
              "      <td>0.645628</td>\n",
              "      <td>0.345232</td>\n",
              "      <td>0.079884</td>\n",
              "      <td>-0.411670</td>\n",
              "      <td>0.391415</td>\n",
              "      <td>0.183423</td>\n",
              "      <td>-0.235963</td>\n",
              "      <td>-0.161149</td>\n",
              "      <td>-0.041473</td>\n",
              "      <td>-0.020309</td>\n",
              "      <td>-0.019613</td>\n",
              "      <td>0.023619</td>\n",
              "      <td>-0.246400</td>\n",
              "      <td>0.067217</td>\n",
              "      <td>-0.304000</td>\n",
              "      <td>-0.151113</td>\n",
              "      <td>0.311132</td>\n",
              "      <td>0.268303</td>\n",
              "      <td>0.286366</td>\n",
              "      <td>-0.657279</td>\n",
              "      <td>0.296580</td>\n",
              "      <td>-0.297199</td>\n",
              "      <td>0.170490</td>\n",
              "      <td>0.009121</td>\n",
              "      <td>0.309414</td>\n",
              "      <td>-0.074530</td>\n",
              "      <td>0.039092</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.087832</td>\n",
              "      <td>-0.107093</td>\n",
              "      <td>-0.391167</td>\n",
              "      <td>0.284849</td>\n",
              "      <td>-0.634053</td>\n",
              "      <td>-0.000102</td>\n",
              "      <td>-0.110829</td>\n",
              "      <td>-0.066894</td>\n",
              "      <td>-0.119985</td>\n",
              "      <td>0.330882</td>\n",
              "      <td>-0.173505</td>\n",
              "      <td>0.815177</td>\n",
              "      <td>0.137314</td>\n",
              "      <td>-0.015449</td>\n",
              "      <td>0.370985</td>\n",
              "      <td>0.535364</td>\n",
              "      <td>0.279643</td>\n",
              "      <td>-0.309391</td>\n",
              "      <td>0.080370</td>\n",
              "      <td>0.285902</td>\n",
              "      <td>0.202510</td>\n",
              "      <td>-0.044770</td>\n",
              "      <td>0.098801</td>\n",
              "      <td>-0.003973</td>\n",
              "      <td>-7.350491</td>\n",
              "      <td>-0.443077</td>\n",
              "      <td>-0.287890</td>\n",
              "      <td>-0.112476</td>\n",
              "      <td>0.446208</td>\n",
              "      <td>-0.126324</td>\n",
              "      <td>0.109541</td>\n",
              "      <td>-0.037666</td>\n",
              "      <td>0.207518</td>\n",
              "      <td>-0.161789</td>\n",
              "      <td>-0.045041</td>\n",
              "      <td>-0.364930</td>\n",
              "      <td>-0.058631</td>\n",
              "      <td>-0.290070</td>\n",
              "      <td>0.260969</td>\n",
              "      <td>0.032437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1367</th>\n",
              "      <td>-0.123137</td>\n",
              "      <td>0.161302</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.030448</td>\n",
              "      <td>-0.247531</td>\n",
              "      <td>-0.072720</td>\n",
              "      <td>-0.001706</td>\n",
              "      <td>0.269038</td>\n",
              "      <td>-0.058014</td>\n",
              "      <td>-0.132209</td>\n",
              "      <td>-0.267632</td>\n",
              "      <td>-0.071179</td>\n",
              "      <td>-0.041266</td>\n",
              "      <td>0.076731</td>\n",
              "      <td>0.197315</td>\n",
              "      <td>-0.004503</td>\n",
              "      <td>-0.083746</td>\n",
              "      <td>0.131318</td>\n",
              "      <td>0.069784</td>\n",
              "      <td>-0.273903</td>\n",
              "      <td>-0.158663</td>\n",
              "      <td>0.115208</td>\n",
              "      <td>-0.134675</td>\n",
              "      <td>-0.274789</td>\n",
              "      <td>-0.061994</td>\n",
              "      <td>-0.190785</td>\n",
              "      <td>0.135280</td>\n",
              "      <td>-0.130710</td>\n",
              "      <td>-0.063052</td>\n",
              "      <td>0.392151</td>\n",
              "      <td>0.115147</td>\n",
              "      <td>-0.035766</td>\n",
              "      <td>-0.081318</td>\n",
              "      <td>0.075713</td>\n",
              "      <td>-0.235339</td>\n",
              "      <td>-0.137516</td>\n",
              "      <td>0.044584</td>\n",
              "      <td>0.175271</td>\n",
              "      <td>0.048209</td>\n",
              "      <td>0.023405</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.226708</td>\n",
              "      <td>-0.163739</td>\n",
              "      <td>-0.072251</td>\n",
              "      <td>0.484708</td>\n",
              "      <td>-0.154423</td>\n",
              "      <td>-0.084078</td>\n",
              "      <td>-0.097128</td>\n",
              "      <td>0.017246</td>\n",
              "      <td>-0.120185</td>\n",
              "      <td>0.007282</td>\n",
              "      <td>0.094375</td>\n",
              "      <td>0.110626</td>\n",
              "      <td>-0.089989</td>\n",
              "      <td>-0.049316</td>\n",
              "      <td>0.191785</td>\n",
              "      <td>0.142130</td>\n",
              "      <td>0.283179</td>\n",
              "      <td>0.051745</td>\n",
              "      <td>0.006025</td>\n",
              "      <td>0.050946</td>\n",
              "      <td>-0.204614</td>\n",
              "      <td>-0.194649</td>\n",
              "      <td>0.137897</td>\n",
              "      <td>0.098931</td>\n",
              "      <td>-8.540627</td>\n",
              "      <td>-0.093423</td>\n",
              "      <td>-0.158638</td>\n",
              "      <td>-0.279031</td>\n",
              "      <td>0.166203</td>\n",
              "      <td>0.141452</td>\n",
              "      <td>0.096975</td>\n",
              "      <td>-0.035828</td>\n",
              "      <td>0.079397</td>\n",
              "      <td>-0.009008</td>\n",
              "      <td>0.001281</td>\n",
              "      <td>0.027574</td>\n",
              "      <td>0.050122</td>\n",
              "      <td>-0.026519</td>\n",
              "      <td>0.298698</td>\n",
              "      <td>0.438693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3804</th>\n",
              "      <td>0.064484</td>\n",
              "      <td>0.112836</td>\n",
              "      <td>0.184944</td>\n",
              "      <td>-0.029359</td>\n",
              "      <td>-0.230980</td>\n",
              "      <td>-0.399629</td>\n",
              "      <td>0.104782</td>\n",
              "      <td>0.246633</td>\n",
              "      <td>0.215279</td>\n",
              "      <td>-0.277077</td>\n",
              "      <td>0.014187</td>\n",
              "      <td>-0.015833</td>\n",
              "      <td>-0.013425</td>\n",
              "      <td>0.155254</td>\n",
              "      <td>0.019352</td>\n",
              "      <td>-0.151311</td>\n",
              "      <td>-0.204869</td>\n",
              "      <td>0.034749</td>\n",
              "      <td>0.148587</td>\n",
              "      <td>-0.003532</td>\n",
              "      <td>0.041445</td>\n",
              "      <td>-0.014028</td>\n",
              "      <td>-0.033731</td>\n",
              "      <td>-0.103968</td>\n",
              "      <td>0.036191</td>\n",
              "      <td>-0.138556</td>\n",
              "      <td>0.053588</td>\n",
              "      <td>-0.367702</td>\n",
              "      <td>0.245672</td>\n",
              "      <td>0.172870</td>\n",
              "      <td>0.050236</td>\n",
              "      <td>-0.178078</td>\n",
              "      <td>-0.400897</td>\n",
              "      <td>-0.152786</td>\n",
              "      <td>0.042996</td>\n",
              "      <td>0.077262</td>\n",
              "      <td>-0.090309</td>\n",
              "      <td>0.091304</td>\n",
              "      <td>-0.078686</td>\n",
              "      <td>-0.149808</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044403</td>\n",
              "      <td>-0.114059</td>\n",
              "      <td>-0.164455</td>\n",
              "      <td>-0.052681</td>\n",
              "      <td>-0.207475</td>\n",
              "      <td>0.016595</td>\n",
              "      <td>0.089094</td>\n",
              "      <td>-0.374178</td>\n",
              "      <td>-0.070617</td>\n",
              "      <td>-0.093807</td>\n",
              "      <td>-0.167856</td>\n",
              "      <td>0.345741</td>\n",
              "      <td>-0.064925</td>\n",
              "      <td>0.119203</td>\n",
              "      <td>-0.004075</td>\n",
              "      <td>0.504372</td>\n",
              "      <td>0.305251</td>\n",
              "      <td>0.038028</td>\n",
              "      <td>-0.108098</td>\n",
              "      <td>0.030905</td>\n",
              "      <td>0.119663</td>\n",
              "      <td>-0.160355</td>\n",
              "      <td>-0.059540</td>\n",
              "      <td>0.008232</td>\n",
              "      <td>-8.227970</td>\n",
              "      <td>-0.188189</td>\n",
              "      <td>-0.182513</td>\n",
              "      <td>0.006779</td>\n",
              "      <td>-0.134192</td>\n",
              "      <td>0.105130</td>\n",
              "      <td>-0.412277</td>\n",
              "      <td>-0.028047</td>\n",
              "      <td>-0.107278</td>\n",
              "      <td>0.025382</td>\n",
              "      <td>-0.084332</td>\n",
              "      <td>0.036818</td>\n",
              "      <td>-0.035262</td>\n",
              "      <td>-0.282251</td>\n",
              "      <td>0.254791</td>\n",
              "      <td>0.416548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1740</th>\n",
              "      <td>-0.135182</td>\n",
              "      <td>0.243332</td>\n",
              "      <td>0.037387</td>\n",
              "      <td>-0.101259</td>\n",
              "      <td>-0.225128</td>\n",
              "      <td>-0.232087</td>\n",
              "      <td>0.103272</td>\n",
              "      <td>0.585362</td>\n",
              "      <td>-0.147522</td>\n",
              "      <td>0.158251</td>\n",
              "      <td>-0.096809</td>\n",
              "      <td>-0.008860</td>\n",
              "      <td>-0.012228</td>\n",
              "      <td>0.251811</td>\n",
              "      <td>0.104072</td>\n",
              "      <td>-0.168540</td>\n",
              "      <td>-0.245230</td>\n",
              "      <td>0.194891</td>\n",
              "      <td>-0.108581</td>\n",
              "      <td>-0.202534</td>\n",
              "      <td>0.048835</td>\n",
              "      <td>-0.126133</td>\n",
              "      <td>-0.100097</td>\n",
              "      <td>0.008151</td>\n",
              "      <td>-0.235607</td>\n",
              "      <td>-0.088785</td>\n",
              "      <td>0.040521</td>\n",
              "      <td>-0.202021</td>\n",
              "      <td>0.117896</td>\n",
              "      <td>0.154183</td>\n",
              "      <td>0.229348</td>\n",
              "      <td>-0.029571</td>\n",
              "      <td>-0.198610</td>\n",
              "      <td>0.123274</td>\n",
              "      <td>-0.232734</td>\n",
              "      <td>0.119141</td>\n",
              "      <td>-0.014998</td>\n",
              "      <td>0.174178</td>\n",
              "      <td>-0.088709</td>\n",
              "      <td>0.059260</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008836</td>\n",
              "      <td>-0.015933</td>\n",
              "      <td>-0.251765</td>\n",
              "      <td>0.137402</td>\n",
              "      <td>-0.337371</td>\n",
              "      <td>0.078837</td>\n",
              "      <td>-0.027392</td>\n",
              "      <td>-0.205486</td>\n",
              "      <td>0.064230</td>\n",
              "      <td>-0.089911</td>\n",
              "      <td>-0.145390</td>\n",
              "      <td>0.273250</td>\n",
              "      <td>-0.385526</td>\n",
              "      <td>-0.108535</td>\n",
              "      <td>0.092317</td>\n",
              "      <td>0.321951</td>\n",
              "      <td>0.001874</td>\n",
              "      <td>0.031806</td>\n",
              "      <td>0.027889</td>\n",
              "      <td>-0.099824</td>\n",
              "      <td>0.032487</td>\n",
              "      <td>-0.191347</td>\n",
              "      <td>-0.247185</td>\n",
              "      <td>0.049384</td>\n",
              "      <td>-9.169405</td>\n",
              "      <td>-0.310088</td>\n",
              "      <td>-0.207202</td>\n",
              "      <td>-0.223072</td>\n",
              "      <td>0.027044</td>\n",
              "      <td>-0.146237</td>\n",
              "      <td>0.172182</td>\n",
              "      <td>-0.081153</td>\n",
              "      <td>0.107697</td>\n",
              "      <td>-0.044608</td>\n",
              "      <td>0.081459</td>\n",
              "      <td>-0.112823</td>\n",
              "      <td>-0.055023</td>\n",
              "      <td>-0.107039</td>\n",
              "      <td>0.155253</td>\n",
              "      <td>0.142687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4178</th>\n",
              "      <td>0.022055</td>\n",
              "      <td>0.228370</td>\n",
              "      <td>0.105429</td>\n",
              "      <td>0.013299</td>\n",
              "      <td>-0.290887</td>\n",
              "      <td>-0.059274</td>\n",
              "      <td>0.294259</td>\n",
              "      <td>0.177698</td>\n",
              "      <td>0.054029</td>\n",
              "      <td>-0.105190</td>\n",
              "      <td>0.095104</td>\n",
              "      <td>-0.041818</td>\n",
              "      <td>0.125372</td>\n",
              "      <td>0.203721</td>\n",
              "      <td>0.173718</td>\n",
              "      <td>0.180575</td>\n",
              "      <td>-0.228251</td>\n",
              "      <td>0.256252</td>\n",
              "      <td>0.119362</td>\n",
              "      <td>0.005637</td>\n",
              "      <td>-0.063715</td>\n",
              "      <td>-0.191752</td>\n",
              "      <td>0.003547</td>\n",
              "      <td>-0.115926</td>\n",
              "      <td>0.211586</td>\n",
              "      <td>0.044368</td>\n",
              "      <td>-0.047643</td>\n",
              "      <td>-0.250465</td>\n",
              "      <td>-0.016332</td>\n",
              "      <td>0.048314</td>\n",
              "      <td>-0.079169</td>\n",
              "      <td>0.161522</td>\n",
              "      <td>0.039788</td>\n",
              "      <td>0.028078</td>\n",
              "      <td>0.071952</td>\n",
              "      <td>-0.028610</td>\n",
              "      <td>0.085328</td>\n",
              "      <td>-0.098000</td>\n",
              "      <td>0.079027</td>\n",
              "      <td>0.106937</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.130266</td>\n",
              "      <td>0.193201</td>\n",
              "      <td>-0.082142</td>\n",
              "      <td>0.169832</td>\n",
              "      <td>-0.097225</td>\n",
              "      <td>0.160290</td>\n",
              "      <td>-0.008620</td>\n",
              "      <td>-0.043932</td>\n",
              "      <td>-0.088044</td>\n",
              "      <td>-0.064925</td>\n",
              "      <td>-0.052666</td>\n",
              "      <td>0.217033</td>\n",
              "      <td>0.203030</td>\n",
              "      <td>0.098908</td>\n",
              "      <td>0.071524</td>\n",
              "      <td>0.347245</td>\n",
              "      <td>0.322668</td>\n",
              "      <td>-0.017341</td>\n",
              "      <td>0.181028</td>\n",
              "      <td>-0.019801</td>\n",
              "      <td>-0.409641</td>\n",
              "      <td>-0.079416</td>\n",
              "      <td>0.095269</td>\n",
              "      <td>0.216242</td>\n",
              "      <td>-9.063298</td>\n",
              "      <td>-0.170920</td>\n",
              "      <td>-0.027152</td>\n",
              "      <td>-0.202578</td>\n",
              "      <td>-0.049786</td>\n",
              "      <td>-0.095463</td>\n",
              "      <td>-0.055699</td>\n",
              "      <td>-0.151857</td>\n",
              "      <td>-0.116684</td>\n",
              "      <td>-0.011671</td>\n",
              "      <td>0.131622</td>\n",
              "      <td>0.207884</td>\n",
              "      <td>-0.044799</td>\n",
              "      <td>-0.250021</td>\n",
              "      <td>0.060028</td>\n",
              "      <td>0.338161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525</th>\n",
              "      <td>0.094541</td>\n",
              "      <td>0.150615</td>\n",
              "      <td>-0.049069</td>\n",
              "      <td>-0.617262</td>\n",
              "      <td>-0.095775</td>\n",
              "      <td>-0.367379</td>\n",
              "      <td>0.665938</td>\n",
              "      <td>0.877984</td>\n",
              "      <td>0.136461</td>\n",
              "      <td>0.074273</td>\n",
              "      <td>0.027263</td>\n",
              "      <td>-0.128320</td>\n",
              "      <td>-0.166325</td>\n",
              "      <td>0.415923</td>\n",
              "      <td>0.181325</td>\n",
              "      <td>-0.410828</td>\n",
              "      <td>-0.373523</td>\n",
              "      <td>0.297706</td>\n",
              "      <td>-0.012054</td>\n",
              "      <td>-0.034615</td>\n",
              "      <td>0.271161</td>\n",
              "      <td>-0.267382</td>\n",
              "      <td>-0.061399</td>\n",
              "      <td>0.119094</td>\n",
              "      <td>0.495991</td>\n",
              "      <td>-0.341781</td>\n",
              "      <td>-0.187930</td>\n",
              "      <td>0.156939</td>\n",
              "      <td>-0.141724</td>\n",
              "      <td>-0.208189</td>\n",
              "      <td>0.096634</td>\n",
              "      <td>0.250226</td>\n",
              "      <td>-0.555547</td>\n",
              "      <td>-0.031423</td>\n",
              "      <td>0.409983</td>\n",
              "      <td>-0.030672</td>\n",
              "      <td>0.426770</td>\n",
              "      <td>-0.202668</td>\n",
              "      <td>0.009666</td>\n",
              "      <td>0.383169</td>\n",
              "      <td>...</td>\n",
              "      <td>0.358575</td>\n",
              "      <td>-0.469684</td>\n",
              "      <td>-0.078363</td>\n",
              "      <td>0.179782</td>\n",
              "      <td>-0.062992</td>\n",
              "      <td>0.136714</td>\n",
              "      <td>0.263178</td>\n",
              "      <td>0.078512</td>\n",
              "      <td>0.331441</td>\n",
              "      <td>-0.129152</td>\n",
              "      <td>-0.236196</td>\n",
              "      <td>0.525067</td>\n",
              "      <td>-0.134202</td>\n",
              "      <td>-0.041980</td>\n",
              "      <td>-0.168327</td>\n",
              "      <td>0.920771</td>\n",
              "      <td>0.304712</td>\n",
              "      <td>0.155946</td>\n",
              "      <td>0.201304</td>\n",
              "      <td>-0.224251</td>\n",
              "      <td>0.035125</td>\n",
              "      <td>-0.438488</td>\n",
              "      <td>0.122343</td>\n",
              "      <td>0.153306</td>\n",
              "      <td>-7.253574</td>\n",
              "      <td>-0.021604</td>\n",
              "      <td>0.183386</td>\n",
              "      <td>-0.124922</td>\n",
              "      <td>-0.077415</td>\n",
              "      <td>-0.669127</td>\n",
              "      <td>0.162543</td>\n",
              "      <td>-0.427148</td>\n",
              "      <td>0.004648</td>\n",
              "      <td>-0.324296</td>\n",
              "      <td>0.187049</td>\n",
              "      <td>-0.178941</td>\n",
              "      <td>-0.271060</td>\n",
              "      <td>0.006874</td>\n",
              "      <td>0.486896</td>\n",
              "      <td>0.389604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2368</th>\n",
              "      <td>0.130424</td>\n",
              "      <td>-0.152382</td>\n",
              "      <td>-0.130404</td>\n",
              "      <td>-0.297400</td>\n",
              "      <td>-0.677944</td>\n",
              "      <td>0.095679</td>\n",
              "      <td>0.645395</td>\n",
              "      <td>0.514784</td>\n",
              "      <td>-0.252474</td>\n",
              "      <td>-0.164425</td>\n",
              "      <td>0.318387</td>\n",
              "      <td>0.327145</td>\n",
              "      <td>0.313982</td>\n",
              "      <td>0.425076</td>\n",
              "      <td>0.442580</td>\n",
              "      <td>0.174750</td>\n",
              "      <td>-0.083539</td>\n",
              "      <td>0.234569</td>\n",
              "      <td>-0.001177</td>\n",
              "      <td>-0.467634</td>\n",
              "      <td>-0.257922</td>\n",
              "      <td>-0.228153</td>\n",
              "      <td>0.138250</td>\n",
              "      <td>0.159996</td>\n",
              "      <td>-0.340688</td>\n",
              "      <td>0.252778</td>\n",
              "      <td>0.060795</td>\n",
              "      <td>0.007899</td>\n",
              "      <td>0.256134</td>\n",
              "      <td>0.059065</td>\n",
              "      <td>-0.107853</td>\n",
              "      <td>0.206987</td>\n",
              "      <td>-0.229164</td>\n",
              "      <td>0.005189</td>\n",
              "      <td>0.102528</td>\n",
              "      <td>-0.307395</td>\n",
              "      <td>0.743267</td>\n",
              "      <td>0.007362</td>\n",
              "      <td>-0.163492</td>\n",
              "      <td>-0.054449</td>\n",
              "      <td>...</td>\n",
              "      <td>0.302810</td>\n",
              "      <td>-0.479928</td>\n",
              "      <td>0.030011</td>\n",
              "      <td>0.267318</td>\n",
              "      <td>-0.110930</td>\n",
              "      <td>-0.411906</td>\n",
              "      <td>0.286018</td>\n",
              "      <td>-0.234996</td>\n",
              "      <td>0.049273</td>\n",
              "      <td>-0.217242</td>\n",
              "      <td>-0.332336</td>\n",
              "      <td>0.331813</td>\n",
              "      <td>0.506870</td>\n",
              "      <td>-0.289381</td>\n",
              "      <td>0.088521</td>\n",
              "      <td>0.602075</td>\n",
              "      <td>0.053710</td>\n",
              "      <td>0.112821</td>\n",
              "      <td>0.548278</td>\n",
              "      <td>-0.160381</td>\n",
              "      <td>0.162321</td>\n",
              "      <td>-0.301806</td>\n",
              "      <td>-0.256764</td>\n",
              "      <td>0.359652</td>\n",
              "      <td>-7.515380</td>\n",
              "      <td>0.116721</td>\n",
              "      <td>0.424066</td>\n",
              "      <td>-0.888678</td>\n",
              "      <td>-0.031682</td>\n",
              "      <td>-0.171103</td>\n",
              "      <td>-0.344377</td>\n",
              "      <td>0.032755</td>\n",
              "      <td>0.144914</td>\n",
              "      <td>0.026463</td>\n",
              "      <td>0.353183</td>\n",
              "      <td>0.001969</td>\n",
              "      <td>-0.162522</td>\n",
              "      <td>-0.291344</td>\n",
              "      <td>0.395167</td>\n",
              "      <td>0.241251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1634</th>\n",
              "      <td>0.324212</td>\n",
              "      <td>0.219663</td>\n",
              "      <td>0.189598</td>\n",
              "      <td>0.066292</td>\n",
              "      <td>-0.019593</td>\n",
              "      <td>-0.407157</td>\n",
              "      <td>0.086597</td>\n",
              "      <td>0.503482</td>\n",
              "      <td>0.085486</td>\n",
              "      <td>-0.291093</td>\n",
              "      <td>0.354070</td>\n",
              "      <td>-0.293304</td>\n",
              "      <td>0.085690</td>\n",
              "      <td>0.100947</td>\n",
              "      <td>0.098075</td>\n",
              "      <td>0.011470</td>\n",
              "      <td>-0.353805</td>\n",
              "      <td>0.424142</td>\n",
              "      <td>0.169108</td>\n",
              "      <td>-0.178416</td>\n",
              "      <td>0.103044</td>\n",
              "      <td>-0.075770</td>\n",
              "      <td>-0.054343</td>\n",
              "      <td>-0.212504</td>\n",
              "      <td>0.147647</td>\n",
              "      <td>-0.109887</td>\n",
              "      <td>-0.075589</td>\n",
              "      <td>-0.208288</td>\n",
              "      <td>0.213443</td>\n",
              "      <td>0.089908</td>\n",
              "      <td>0.242875</td>\n",
              "      <td>0.279945</td>\n",
              "      <td>-0.223174</td>\n",
              "      <td>0.021398</td>\n",
              "      <td>0.111597</td>\n",
              "      <td>-0.239140</td>\n",
              "      <td>0.202744</td>\n",
              "      <td>-0.120143</td>\n",
              "      <td>0.151196</td>\n",
              "      <td>0.117123</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026804</td>\n",
              "      <td>-0.155791</td>\n",
              "      <td>-0.020570</td>\n",
              "      <td>0.155556</td>\n",
              "      <td>-0.218920</td>\n",
              "      <td>-0.015522</td>\n",
              "      <td>0.119135</td>\n",
              "      <td>-0.248992</td>\n",
              "      <td>-0.231950</td>\n",
              "      <td>-0.169696</td>\n",
              "      <td>-0.192221</td>\n",
              "      <td>0.214063</td>\n",
              "      <td>-0.056670</td>\n",
              "      <td>0.160972</td>\n",
              "      <td>-0.479516</td>\n",
              "      <td>0.486994</td>\n",
              "      <td>0.108656</td>\n",
              "      <td>0.179402</td>\n",
              "      <td>0.100553</td>\n",
              "      <td>-0.033333</td>\n",
              "      <td>-0.080456</td>\n",
              "      <td>0.104493</td>\n",
              "      <td>-0.295297</td>\n",
              "      <td>0.328605</td>\n",
              "      <td>-7.770480</td>\n",
              "      <td>0.238563</td>\n",
              "      <td>-0.043716</td>\n",
              "      <td>-0.066590</td>\n",
              "      <td>-0.328754</td>\n",
              "      <td>-0.508589</td>\n",
              "      <td>-0.286559</td>\n",
              "      <td>-0.177503</td>\n",
              "      <td>0.151620</td>\n",
              "      <td>-0.188073</td>\n",
              "      <td>0.286500</td>\n",
              "      <td>0.151631</td>\n",
              "      <td>-0.001627</td>\n",
              "      <td>0.039074</td>\n",
              "      <td>0.284077</td>\n",
              "      <td>0.314951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1558</th>\n",
              "      <td>0.029094</td>\n",
              "      <td>0.431916</td>\n",
              "      <td>-0.040330</td>\n",
              "      <td>-0.271382</td>\n",
              "      <td>-0.285266</td>\n",
              "      <td>-0.460819</td>\n",
              "      <td>0.333145</td>\n",
              "      <td>0.466775</td>\n",
              "      <td>0.316905</td>\n",
              "      <td>-0.164273</td>\n",
              "      <td>0.036707</td>\n",
              "      <td>-0.189330</td>\n",
              "      <td>0.194290</td>\n",
              "      <td>-0.139596</td>\n",
              "      <td>0.170976</td>\n",
              "      <td>-0.209974</td>\n",
              "      <td>-0.379666</td>\n",
              "      <td>0.375499</td>\n",
              "      <td>0.244904</td>\n",
              "      <td>-0.254573</td>\n",
              "      <td>-0.096786</td>\n",
              "      <td>-0.558578</td>\n",
              "      <td>-0.058257</td>\n",
              "      <td>0.119608</td>\n",
              "      <td>-0.104212</td>\n",
              "      <td>-0.289812</td>\n",
              "      <td>-0.091603</td>\n",
              "      <td>0.054411</td>\n",
              "      <td>0.247683</td>\n",
              "      <td>-0.307580</td>\n",
              "      <td>0.351847</td>\n",
              "      <td>0.294254</td>\n",
              "      <td>-0.343639</td>\n",
              "      <td>0.050585</td>\n",
              "      <td>0.271333</td>\n",
              "      <td>0.248134</td>\n",
              "      <td>0.377017</td>\n",
              "      <td>-0.040673</td>\n",
              "      <td>0.044029</td>\n",
              "      <td>0.247131</td>\n",
              "      <td>...</td>\n",
              "      <td>0.202251</td>\n",
              "      <td>0.150339</td>\n",
              "      <td>-0.065583</td>\n",
              "      <td>0.534417</td>\n",
              "      <td>-0.129610</td>\n",
              "      <td>0.013254</td>\n",
              "      <td>-0.264777</td>\n",
              "      <td>-0.334783</td>\n",
              "      <td>0.227886</td>\n",
              "      <td>-0.060775</td>\n",
              "      <td>0.129266</td>\n",
              "      <td>0.283678</td>\n",
              "      <td>-0.041294</td>\n",
              "      <td>-0.104023</td>\n",
              "      <td>0.333128</td>\n",
              "      <td>0.047045</td>\n",
              "      <td>0.045286</td>\n",
              "      <td>0.473816</td>\n",
              "      <td>0.399066</td>\n",
              "      <td>-0.234576</td>\n",
              "      <td>-0.219675</td>\n",
              "      <td>-0.304301</td>\n",
              "      <td>0.058652</td>\n",
              "      <td>0.183336</td>\n",
              "      <td>-8.575641</td>\n",
              "      <td>-0.565828</td>\n",
              "      <td>-0.292338</td>\n",
              "      <td>-0.363864</td>\n",
              "      <td>-0.040077</td>\n",
              "      <td>-0.319397</td>\n",
              "      <td>0.037829</td>\n",
              "      <td>-0.160836</td>\n",
              "      <td>0.209321</td>\n",
              "      <td>0.316854</td>\n",
              "      <td>-0.247967</td>\n",
              "      <td>-0.210790</td>\n",
              "      <td>-0.020636</td>\n",
              "      <td>-0.299095</td>\n",
              "      <td>0.260910</td>\n",
              "      <td>0.227582</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows Ã— 768 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2    ...       765       766       767\n",
              "3154  0.279578 -0.153533 -0.234437  ... -0.540360  0.218350  0.369504\n",
              "2892 -0.139322  0.050318 -0.050363  ... -0.290070  0.260969  0.032437\n",
              "1367 -0.123137  0.161302  0.001234  ... -0.026519  0.298698  0.438693\n",
              "3804  0.064484  0.112836  0.184944  ... -0.282251  0.254791  0.416548\n",
              "1740 -0.135182  0.243332  0.037387  ... -0.107039  0.155253  0.142687\n",
              "4178  0.022055  0.228370  0.105429  ... -0.250021  0.060028  0.338161\n",
              "525   0.094541  0.150615 -0.049069  ...  0.006874  0.486896  0.389604\n",
              "2368  0.130424 -0.152382 -0.130404  ... -0.291344  0.395167  0.241251\n",
              "1634  0.324212  0.219663  0.189598  ...  0.039074  0.284077  0.314951\n",
              "1558  0.029094  0.431916 -0.040330  ... -0.299095  0.260910  0.227582\n",
              "\n",
              "[10 rows x 768 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRu0cPm9ye5W"
      },
      "source": [
        "# Train/Test splitting\n",
        "from sklearn import model_selection\n",
        "\n",
        "# Hyper-parameter tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV,  GridSearchCV, cross_val_score\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Classifier evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, make_scorer, confusion_matrix, classification_report\n",
        "\n",
        "# Print confusion matrix\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGB99vl7ymWr",
        "outputId": "45994ecb-9ee7-4949-c28b-032010626789"
      },
      "source": [
        "# Print y shape, head, unique values and number of instances in each class\n",
        "print(y.shape)\n",
        "print(y.unique())\n",
        "print(y.value_counts())\n",
        "\n",
        "# split into training and test set\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6354,)\n",
            "[1 2 0]\n",
            "2    2918\n",
            "1    1794\n",
            "0    1642\n",
            "Name: Recipient Gender, dtype: int64\n",
            "(5083, 768)\n",
            "(5083,)\n",
            "(1271, 768)\n",
            "(1271,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngQeGgvbyrrh"
      },
      "source": [
        "def get_best_estimator(clf, params):\n",
        "    # Metrics to use for optimization\n",
        "    # metrics = ['accuracy','f1', 'f1_macro', 'f1_weighted','precision','recall']\n",
        "    \n",
        "    # Optimize based on f1_macro score\n",
        "    grid_search = GridSearchCV(clf, param_grid = params, cv = 3, n_jobs = -1, scoring = 'f1_macro', verbose=2)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    \n",
        "    # Get estimator with best parameter valuese\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "    print(best_estimator)\n",
        "    \n",
        "    # Return best_estimator\n",
        "    return best_estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GECsXw3WzIWo"
      },
      "source": [
        "def print_results(clf, y_pred):\n",
        "    # Print accuracy score \n",
        "    print(\"Accuracy Score -> \",accuracy_score(y_pred, y_test)*100)\n",
        "    \n",
        "    # Print confusion matrix (heat map)\n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    \n",
        "    # Print avarage 10-fold cross validation accuracy and f1 scores \n",
        "    #svm_cv_accuracy_score = cross_val_score(clf, X_train, y_train, cv=10,scoring='accuracy')\n",
        "    #svm_cv_f1_score = cross_val_score(clf, X_train, y_train, cv=10,scoring='f1_macro')\n",
        "    #print(\"Mean cv accuracy: \", svm_cv_accuracy_score.mean())\n",
        "    #print(\"Mean cv f1_macro: \", svm_cv_f1_score.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srBPju8gzKto"
      },
      "source": [
        "def svm():\n",
        "    # Parameters to optimize for svm\n",
        "    param_grid = {'probability': [True],\n",
        "                  'C': [1, 5, 10],\n",
        "                  'kernel': ['rbf'],\n",
        "                  'gamma': ['auto', 'scale', 0.1, 0.01, 0.001],\n",
        "                 }\n",
        "    # Get optimized parameters from grid search\n",
        "    svm_clf = SVC()\n",
        "    optimized_svm_clf = get_best_estimator(svm_clf, param_grid)\n",
        "    optimized_svm_clf.fit(X_train, y_train)\n",
        "    \n",
        "    # predict recipient gender and print prediction results\n",
        "    y_pred = optimized_svm_clf.predict(X_test)\n",
        "    print_results(optimized_svm_clf, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlKF4rpUzMg5"
      },
      "source": [
        "def random_forest():\n",
        "    # Parameters to optimize for rf\n",
        "    param_grid = {\n",
        "        'max_depth': [10, 50, 100],\n",
        "        'min_samples_leaf': [3, 4],\n",
        "        'min_samples_split': [3, 5],\n",
        "        'n_estimators': [100, 200]\n",
        "    }\n",
        "    \n",
        "    # Get optimized parameters from grid search\n",
        "    rf_clf = RandomForestClassifier()\n",
        "    optimized_rf_clf = get_best_estimator(rf_clf, param_grid)\n",
        "    optimized_rf_clf.fit(X_train, y_train)\n",
        "    \n",
        "    # predict recipient gender and print prediction results\n",
        "    y_pred = optimized_rf_clf.predict(X_test)\n",
        "    print_results(optimized_rf_clf, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3lWxU63zQow"
      },
      "source": [
        "def mlp():\n",
        "    # Parameters to optimize for rf\n",
        "    param_grid = {\n",
        "        'hidden_layer_sizes': [(100,50,10), (100, 50), (20,10,5)],\n",
        "        'activation': ['tanh', 'relu'],\n",
        "        'solver': ['adam'],\n",
        "        'alpha': [0.0001, 0.05],\n",
        "        'max_iter': [500]\n",
        "    }\n",
        "    \n",
        "    # Get optimized parameters from grid search\n",
        "    mlp_clf = MLPClassifier()\n",
        "    optimized_mlp_clf = get_best_estimator(mlp_clf, param_grid)\n",
        "    optimized_mlp_clf.fit(X_train, y_train)\n",
        "    \n",
        "    # predict recipient gender and print prediction results\n",
        "    y_pred = optimized_mlp_clf.predict(X_test)\n",
        "    print_results(optimized_mlp_clf, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "siueuhi7zWcq",
        "outputId": "35f05970-6042-482e-f4eb-e4a10f683cc4"
      },
      "source": [
        "svm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 13.3min\n",
            "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed: 18.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SVC(C=5, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "Accuracy Score ->  57.04169944925256\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.29      0.37       334\n",
            "           1       0.45      0.47      0.46       347\n",
            "           2       0.66      0.79      0.71       590\n",
            "\n",
            "    accuracy                           0.57      1271\n",
            "   macro avg       0.53      0.52      0.51      1271\n",
            "weighted avg       0.56      0.57      0.55      1271\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npUIHgb7zYlt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "bf6cacef-f520-40b4-a0dc-0dc30d18b68a"
      },
      "source": [
        "random_forest()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:  3.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=100, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=3, min_samples_split=5,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n",
            "Accuracy Score ->  51.534225019669556\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.13      0.20       334\n",
            "           1       0.38      0.28      0.32       347\n",
            "           2       0.57      0.87      0.69       590\n",
            "\n",
            "    accuracy                           0.52      1271\n",
            "   macro avg       0.45      0.43      0.40      1271\n",
            "weighted avg       0.47      0.52      0.46      1271\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOjr-i_nzbGL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "f58d6be8-32aa-4eca-8c23-5804dbd68b60"
      },
      "source": [
        "mlp()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  2.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MLPClassifier(activation='relu', alpha=0.05, batch_size='auto', beta_1=0.9,\n",
            "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "              hidden_layer_sizes=(100, 50, 10), learning_rate='constant',\n",
            "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
            "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "              warm_start=False)\n",
            "Accuracy Score ->  49.33123524783635\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.35      0.34       334\n",
            "           1       0.40      0.38      0.39       347\n",
            "           2       0.64      0.64      0.64       590\n",
            "\n",
            "    accuracy                           0.49      1271\n",
            "   macro avg       0.46      0.46      0.46      1271\n",
            "weighted avg       0.49      0.49      0.49      1271\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}